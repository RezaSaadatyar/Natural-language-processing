{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Welcome to Natural language processing (NLP) in Python**<br/>\n",
        "\n",
        "Presented by: Reza Saadatyar (2024-2025)<br/>\n",
        "E-mail: Reza.Saadatyar@outlook.com<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outline:**<br/>\n",
        "1️⃣ <span style=\"font-size:15px; font-weight:bold\">Introduction to NLP</span><br/>\n",
        "▪ Overview of Natural Language Processing<br/>\n",
        "▪ Key areas: Text Analysis, Language Generation, Speech Processing, Semantic Understanding<br/>\n",
        "▪ Introduction to NLTK library and its capabilities<br/>\n",
        "\n",
        "2️⃣ Tokenization Concepts<br/>\n",
        "▪ Definition and importance of tokenization<br/>\n",
        "▪ Types of tokenization: Sentence, Word, Regex, Treebank, WordPunct, Whitespace, Character<br/>\n",
        "▪ Introduction to Webtext Corpus for training tokenizers<br/>\n",
        "\n",
        "3️⃣ Tokenization Examples<br/>\n",
        "▪ Demonstrating different tokenization techniques on sample text<br/>\n",
        "▪ Comparing outputs of various tokenizers (sent_tokenize, word_tokenize, RegexpTokenizer, TreebankWordTokenizer, WordPunctTokenizer, whitespace, character)<br/>\n",
        "\n",
        "4️⃣ Working with External Text Files<br/>\n",
        "▪ Reading and processing text from a file<br/>\n",
        "▪ Analyzing text length and basic properties<br/>\n",
        "\n",
        "5️⃣ Custom Tokenizer Training<br/>\n",
        "▪ Loading and using pre-trained Punkt tokenizer<br/>\n",
        "▪ Training a custom PunktSentenceTokenizer with Webtext corpus data<br/>\n",
        "▪ Comparing pre-trained and custom tokenizer outputs<br/>\n",
        "\n",
        "6️⃣ Practical Applications<br/>\n",
        "▪ Applying tokenization to real-world text data<br/>\n",
        "▪ Understanding the role of tokenization in NLP pipelines<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:#ee0b0b; font-size:20px; font-weight:bold;\">1️⃣ Introduction to NLP</span><br/>\n",
        "NLP is a branch of artificial intelligence that allows computers to understand, interpret, and generate human language by combining linguistics, computer science, and machine learning.<br/>\n",
        "\n",
        "**Key Areas of NLP:**<br/>\n",
        "`Text Analysis:`<br/>\n",
        "▪ Tokenization: Breaking text into words or sentences.<br/>\n",
        "▪ Part-of-Speech (POS) Tagging: Identifying grammatical components (e.g., nouns, verbs).<br/>\n",
        "▪ Named Entity Recognition (NER): Extracting entities like names, dates, or organizations.<br/>\n",
        "▪ Sentiment Analysis: Determining the emotional tone (positive, negative, neutral).<br/>\n",
        "\n",
        "`Language Generation:`<br/>\n",
        "▪ Text Summarization: Condensing long texts into shorter summaries.<br/>\n",
        "▪ Machine Translation: Converting text between languages (e.g., Google Translate).<br/>\n",
        "▪ Text Generation: Creating human-like text (e.g., chatbots, story generators).<br/>\n",
        "\n",
        "`Speech Processing:`<br/>\n",
        "▪ Speech Recognition: Converting spoken words to text (e.g., Siri, Alexa).<br/>\n",
        "▪ Text-to-Speech (TTS): Generating spoken language from text.<br/>\n",
        "▪ Voice Assistants: Combining speech recognition and NLP for interactive systems.<br/>\n",
        "\n",
        "`Semantic Understanding:`<br/>\n",
        "▪ Word Embeddings: Representing words as vectors (e.g., Word2Vec, BERT).<br/>\n",
        "▪ Question Answering: Providing precise answers to user queries.<br/>\n",
        "▪ Dialogue Systems: Enabling conversational agents to maintain context.<br/>\n",
        "\n",
        "**[NLTK](https://www.nltk.org/)**<br/>\n",
        "▪ `Tokenization:` Breaking text into smaller units (words, sentences, or phrases)<br/>\n",
        "▪ `Stemming:` Reducing words to their root form by removing suffixes/prefixes<br/>\n",
        "▪ `Tagging:` Assigning grammatical categories (POS tags) to words<br/>\n",
        "▪ `Parsing:` Analyzing sentence structure and grammatical relationships<br/>\n",
        "▪ `Semantic` Reasoning: Understanding meaning and relationships between words/concepts<br/>\n",
        "▪ `Wrappers:` Interface layers that connect to powerful NLP libraries like spaCy or Stanford NLP<br/>\n",
        "\n",
        "**Custom Tokenizer Training**<br/>\n",
        "We can train our own sentence tokenizer using custom text data. This allows the tokenizer to learn patterns specific to our domain.<br/>\n",
        "\n",
        "[Webtext Corpus](https://paperswithcode.com/dataset/webtext)<br/>\n",
        "The Webtext corpus is a high-quality dataset created by OpenAI through web scraping. It contains diverse text samples that can be used to train robust tokenizers. The corpus emphasizes document quality and natural language patterns.<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:rgb(15, 12, 226); font-size:20px; font-weight:bold;\">Importing libraries</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install nltk\n",
        "import nltk\n",
        "import nltk.data\n",
        "from nltk.corpus import webtext\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer, PunktSentenceTokenizer, WordPunctTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the Punkt tokenizer models:\n",
        "# - 'punkt': Pre-trained sentence tokenizer model\n",
        "# - 'punkt_tab': Additional tokenizer resources for handling special cases\n",
        "# - 'webtext': Dataset containing diverse text samples for training custom tokenizers\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('webtext')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QYEjg91h9Er",
        "outputId": "9d30b798-fc0c-4670-d2c0-83110500712a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence tokenization:\n",
            "['I am learning Natural Language Processing.', \"I'm learning Python programming.\", 'It is very user friendly.', \"I'm ready to start coding.\"]\n",
            "\n",
            "Word tokenization:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.', 'I', \"'m\", 'learning', 'Python', 'programming', '.', 'It', 'is', 'very', 'user', 'friendly', '.', 'I', \"'m\", 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "Regex tokenization (words only):\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', 'I', 'm', 'learning', 'Python', 'programming', 'It', 'is', 'very', 'user', 'friendly', 'I', 'm', 'ready', 'to', 'start', 'coding']\n",
            "\n",
            "TreebankWordTokenizer:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing.', 'I', \"'m\", 'learning', 'Python', 'programming.', 'It', 'is', 'very', 'user', 'friendly.', 'I', \"'m\", 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "WordPunctTokenizer:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.', 'I', \"'\", 'm', 'learning', 'Python', 'programming', '.', 'It', 'is', 'very', 'user', 'friendly', '.', 'I', \"'\", 'm', 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "Whitespace tokenization:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing.', \"I'm\", 'learning', 'Python', 'programming.', 'It', 'is', 'very', 'user', 'friendly.', \"I'm\", 'ready', 'to', 'start', 'coding.']\n",
            "\n",
            "Character tokenization:\n",
            "['I', ' ', 'a', 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '.', ' ', 'I', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'u', 's', 'e', 'r', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 'l', 'y', '.', ' ', 'I', \"'\", 'm', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 's', 't', 'a', 'r', 't', ' ', 'c', 'o', 'd', 'i', 'n', 'g', '.']\n"
          ]
        }
      ],
      "source": [
        "txt = \"I am learning Natural Language Processing. I'm learning Python programming. It is very user friendly. I'm ready to start coding.\"\n",
        "\n",
        "# Using sent_tokenize to split text into sentences\n",
        "# This is useful when you need to process text at the sentence level\n",
        "print(f\"Sentence tokenization:\\n{sent_tokenize(txt)}\")\n",
        "\n",
        "# Using word_tokenize to split text into individual words\n",
        "# This is useful for word-level analysis and processing\n",
        "print(f\"\\nWord tokenization:\\n{word_tokenize(txt)}\")\n",
        "\n",
        "# Using RegexpTokenizer to extract only word characters\n",
        "# This is useful when you want to remove punctuation and keep only alphanumeric characters\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "print(f\"\\nRegex tokenization (words only):\\n{tokenizer.tokenize(txt)}\")\n",
        "\n",
        "# Using TreebankWordTokenizer for standard word tokenization\n",
        "# This follows the Penn Treebank tokenization conventions\n",
        "tree_Toknizer = nltk.TreebankWordTokenizer()\n",
        "print(f\"\\nTreebankWordTokenizer:\\n{tree_Toknizer.tokenize(txt)}\")\n",
        "\n",
        "# Using WordPunctTokenizer to split text into words and punctuation\n",
        "# This is useful when you need to preserve punctuation as separate tokens\n",
        "punkt_token = WordPunctTokenizer()\n",
        "print(f\"\\nWordPunctTokenizer:\\n{punkt_token.tokenize(txt)}\")\n",
        "\n",
        "# Using simple whitespace tokenization\n",
        "# This is the most basic form of tokenization, splitting on spaces\n",
        "print(f\"\\nWhitespace tokenization:\\n{txt.split()}\")\n",
        "\n",
        "# Using character-level tokenization\n",
        "# This is useful for character-level analysis or when working with non-standard text\n",
        "print(f\"\\nCharacter tokenization:\\n{list(txt)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "HMmE-6xjkWl_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Mr reza. How are you today? I can't stand this weather.\n",
            "The sun is too bright and the temperature is unbearable.\n",
            "I don't know how people can work in these conditions.\n",
            "Maybe we should move to a cooler place.\n",
            "What do you think about that?\n"
          ]
        }
      ],
      "source": [
        "# Open a text file using the correct relative path (adjusted for your project structure)\n",
        "txt_file = open(\"D:/Natural-language-processing/Data/sample_text.txt\", mode='r', encoding='utf-8')\n",
        "\n",
        "txt_read = txt_file.read()\n",
        "print(txt_read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eHvSukJmFP1",
        "outputId": "eb5f2781-b851-41c1-e509-5faf1be88311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(txt_read) #the number of charachters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUIDOhZ5oyzy",
        "outputId": "a694b8e6-e4ee-4698-eed6-7542d96cda3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello!',\n",
              " 'Mr reza.',\n",
              " 'How are you today?',\n",
              " \"I can't stand this weather.\",\n",
              " 'The sun is too bright and the temperature is unbearable.',\n",
              " \"I don't know how people can work in these conditions.\",\n",
              " 'Maybe we should move to a cooler place.',\n",
              " 'What do you think about that?']"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the pre-trained English Punkt tokenizer model\n",
        "punkt_tok = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Tokenize the text using the loaded Punkt tokenizer\n",
        "punkt_tok.tokenize(txt_read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVoIEU4lpU1w",
        "outputId": "fb3bcd7f-deca-4f01-c4d0-e98c2dfd799b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the length of the tokenized text using punkt tokenizer\n",
        "len(punkt_tok.tokenize(txt_read))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72xenMzurigM",
        "outputId": "454c1147-e5be-481e-cf2c-93c2f70c1871"
      },
      "outputs": [],
      "source": [
        "# Load raw text data from the 'overheard.txt' file in the webtext corpus\n",
        "text_parameter = webtext.raw('overheard.txt')\n",
        "# print(text_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "K6CLD-5wsW6d"
      },
      "outputs": [],
      "source": [
        "# Create a new instance of PunktSentenceTokenizer and train it with our text data\n",
        "my_tokenizer = PunktSentenceTokenizer(text_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaPhUO8Hsej_",
        "outputId": "b3af1b7e-11cd-4ebb-c8e6-5e3089e1903f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nltk.tokenize.punkt.PunktSentenceTokenizer"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(my_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "C8qhiCQUshUF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pre_token[0]: White guy: So, do you have any plans for this evening?\n",
            "our_token[0]: White guy: So, do you have any plans for this evening?\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text using the pre-trained sent_tokenize function\n",
        "pre_token = sent_tokenize(text_parameter)\n",
        "\n",
        "# Tokenize the text using our custom trained tokenizer\n",
        "our_token = my_tokenizer.tokenize(text_parameter)\n",
        "\n",
        "print(f\"pre_token[0]: {pre_token[0]}\")\n",
        "\n",
        "print(f\"our_token[0]: {our_token[0]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text-Introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
