{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold\">Welcome to Natural language processing (NLP) in Python</span><br/>\n",
        "\n",
        "Presented by: Reza Saadatyar (2024-2025)<br/>\n",
        "E-mail: Reza.Saadatyar@outlook.com<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; font-weight: bold\">Outline:</span><br/>\n",
        "▪ `Introduction NLP`<br/>\n",
        "▪ `Understanding Text Dataset Structure in NLP`<br/>\n",
        "▪ `Semantic, Syntactic, and Sentiment Analysis in NLP`<br/>\n",
        "▪ `Regular Expressions in NLP`<br/>\n",
        "▪ `Stopwords Removal`<br/>\n",
        "▪ `Part-of-Speech (POS) Tagging in NLP`<br/>\n",
        "▪ `Chunking and Parsing in NLP`<br/>\n",
        "▪ `Hypernyms and Hyponyms in NLP`<br/>\n",
        "▪ `Named Entity Recognition (NER)`<br/>\n",
        "▪ `Stemming & Lemmatization`<br/>\n",
        "▪ `Tokenization Concepts`<br/>\n",
        "▪ `Custom Tokenizer Training`<br/>\n",
        "▪ `SpaCy Linguistic Analysis`<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; font-weight: bold; color:rgb(255, 251, 18)\">Introduction NLP</span><br/>\n",
        "NLP is a branch of artificial intelligence that allows computers to understand, interpret, and generate human language by combining linguistics, computer science, and machine learning.<br/> \n",
        "\n",
        "<span style=\"font-size:15.5px; font-weight:bold\">Key Areas of NLP:</span><br/>\n",
        "`Text Analysis:`<br/>\n",
        "▪ Tokenization: Breaking text into words or sentences.<br/>\n",
        "▪ Part-of-Speech (POS) Tagging: Identifying grammatical components (e.g., nouns, verbs).<br/>\n",
        "▪ Named Entity Recognition (NER): Extracting entities like names, dates, or organizations.<br/>\n",
        "▪ Sentiment Analysis: Determining the emotional tone (positive, negative, neutral).<br/>\n",
        "\n",
        "`Language Generation:`<br/>\n",
        "▪ Text Summarization: Condensing long texts into shorter summaries.<br/>\n",
        "▪ Machine Translation: Converting text between languages (e.g., Google Translate).<br/>\n",
        "▪ Text Generation: Creating human-like text (e.g., chatbots, story generators).<br/>\n",
        "\n",
        "`Speech Processing:`<br/>\n",
        "▪ Speech Recognition: Converting spoken words to text (e.g., Siri, Alexa).<br/>\n",
        "▪ Text-to-Speech (TTS): Generating spoken language from text.<br/>\n",
        "▪ Voice Assistants: Combining speech recognition and NLP for interactive systems.<br/>\n",
        "\n",
        "`Semantic Understanding:`<br/>\n",
        "▪ Word Embeddings: Representing words as vectors (e.g., Word2Vec, BERT).<br/>\n",
        "▪ Question Answering: Providing precise answers to user queries.<br/>\n",
        "▪ Dialogue Systems: Enabling conversational agents to maintain context.<br/>\n",
        "\n",
        "<span style=\"font-size:15.5px; font-weight:bold\">NLP Challenges:</span><br/>\n",
        "▪ `Ambiguity & Context Sensitivity:` Human language is ambiguous; words or sentences can have multiple meanings, so NLP systems must use context to interpret them correctly.<br/>\n",
        "▪ `Cultural & Linguistic Nuances:` Cultural, regional, and linguistic differences—such as idioms, slang, and unique grammar—make accurate language interpretation challenging for NLP systems.<br/>\n",
        "▪ `Handling Massive Datasets:` NLP must efficiently handle and analyze large-scale text data, requiring fast algorithms and scalable systems.<br/>\n",
        "▪ `Continuous Innovation for Greater Accuracy & Relevance:`NLP is constantly advancing, requiring ongoing updates and improvements to maintain effective and reliable systems.<br/>\n",
        "\n",
        "<span style=\"font-size:15.5px; font-weight:bold;\">NLTK and spaCy libraries</span><br/>\n",
        "▪ [NLTK](https://www.nltk.org/) is a popular open-source Python library for NLP, offering essential tools (tokenization, stemming, lemmatization, parsing) and access to rich linguistic datasets for research and education.<br/>\n",
        "▪ [spaCy](https://spacy.io/) is a fast, modern Python NLP library designed for practical, large-scale text processing. It excels at tasks like tokenization, named entity recognition, and dependency parsing, making it ideal for real-world applications.<br/>\n",
        "\n",
        "• `Classification:` Assigning predefined categories or labels to text, such as spam detection, sentiment analysis, or topic categorization.<br/>\n",
        "• `Tokenization:` Breaking text into smaller units (words, sentences, or phrases)<br/>\n",
        "• `Stemming:` Reducing words to their root form by removing suffixes/prefixes<br/>\n",
        "• `Tagging:` Assigning grammatical categories (POS tags) to words<br/>\n",
        "• `Parsing:` Analyzing sentence structure and grammatical relationships<br/>\n",
        "• `Semantic Reasoning:` Understanding meaning and relationships between words/concepts<br/>\n",
        "• `Wrappers:` Interface layers that connect to powerful NLP libraries like spaCy or Stanford NLP<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:15.5px; font-weight:bold\">NLTK & spaCy Setup:</span><br/>\n",
        "• `NLTK's Punkt` is an unsupervised sentence tokenizer that splits text into sentences based on learned punctuation and abbreviation patterns. spaCy offers its own fast, statistical sentence segmentation for diverse text types.<br/>\n",
        "• `en_core_web_sm` is spaCy's lightweight English model for basic NLP tasks (tokenization, POS tagging, parsing, NER). For higher accuracy or advanced features, use larger models like `en_core_web_md`, `en_core_web_lg`, or `en_core_web_trf`.<br/>\n",
        "• `WordNet` in NLTK groups English words into synonym sets (synsets) for semantic analysis. Alternatives like `BabelNet` and `ConceptNet` offer broader, multilingual semantic networks.<br/>\n",
        "• The `Gutenberg` corpus in NLTK contains classic literary texts from Project Gutenberg, ideal for studying historical language. Other corpora like `Brown` or `Reuters` offer different genres and perspectives for text analysis.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install spacy\n",
        "! pip install nltk\n",
        "! pip install regex==2023.10.3\n",
        "! pip install spacy-wordnet\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================= Download required NLTK data ========================================\n",
        "nltk.download('punkt')      # Download the pre-trained sentence tokenizer model ('punkt')\n",
        "nltk.download('punkt_tab')  # Download additional tokenizer resources for handling special cases ('punkt_tab')\n",
        "nltk.download('wordnet')    # Download the WordNet lexical database for lemmatization\n",
        "nltk.download('webtext')    # Download the Webtext corpus containing diverse text samples\n",
        "nltk.download('gutenberg')  # Download the Gutenberg corpus for text samples\n",
        "nltk.download('stopwords')  # Download the list of common stopwords\n",
        "nltk.download('vader_lexicon')  # Download the VADER sentiment analysis lexicon\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Download the averaged perceptron tagger for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)  # Download the averaged perceptron tagger for POS tagging\n",
        "python -m spacy download en_core_web_sm  # Download the small English model for spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<span style=\"font-size: 16px; color: rgb(11, 7, 241); font-weight: bold\">Understanding Text Dataset Structure in NLP</span><br/>\n",
        "▪ `Corpora:` Collections of large, structured text datasets used for linguistic analysis or training NLP models. A corpus can contain thousands or millions of documents (multiple datasets).<br/>\n",
        "▪ `Corpus:` Sometimes, the term \"corpus\" is used to refer to a single collection of documents, while \"corpora\" refers to multiple such collections (one dataset).<br/>\n",
        "▪ `Document:` A document is an individual piece of text within a corpus, such as an article, a book, a tweet, or an email (one text).<br/>\n",
        "▪ `Token:`A token is the smallest unit of text, typically a word, punctuation mark, or symbol, obtained after tokenization. Tokenization is the process of splitting text into these basic units (word, punctuation, etc.).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents in the webtext corpus:\n",
            "• firefox.txt\n",
            "• grail.txt\n",
            "• overheard.txt\n",
            "• pirates.txt\n",
            "• singles.txt\n",
            "• wine.txt\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section shows how to access and explore a real-world text corpus using NLTK. \n",
        "- It introduces the concepts of corpus and document, demonstrates how to list available documents in the 'webtext' \n",
        "corpus, and prepares for further text processing by loading and inspecting sample data.\n",
        "'''\n",
        "\n",
        "from nltk.corpus import webtext # Provides access to the Webtext corpus, useful for training and testing tokenizers\n",
        "\n",
        "corpus = webtext # The entire webtext corpus\n",
        "documents = corpus.fileids()   # List all documents (fileids) in the corpus\n",
        "print(\"Documents in the webtext corpus:\")\n",
        "for doc in documents:\n",
        "    print(f\"• {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Document: firefox.txt\n",
            "\n",
            "First 300 characters of the document:\n",
            "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
            "When in full screen mode\n",
            "Pressing Ctrl-N should open a new browser when only download dialog is left open\n",
            "add icons to context menu\n",
            "So called \"tab bar\" should be made a proper toolbar or given \n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Select and preview a document from the webtext corpus by printing its ID and the first 300 characters.\n",
        "'''\n",
        "\n",
        "document_id = documents[0]  # Select a document (e.g., 'grail.txt')\n",
        "print(f\"Selected Document: {document_id}\")\n",
        "\n",
        "raw_text = corpus.raw(document_id)  # Get the raw text of the document\n",
        "print(f\"\\nFirst 300 characters of the document:\\n{raw_text[:300]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences in the document: 1142\n",
            "\n",
            "First sentence:\n",
            "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
            "When in full screen mode\n",
            "Pressing Ctrl-N should open a new browser when only download dialog is left open\n",
            "add icons to context menu\n",
            "So called \"tab bar\" should be made a proper toolbar or given the ability collapse / expand.\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section Show how to use the `sent_tokenize` function to break raw text into individual sentences.\n",
        "'''\n",
        "\n",
        "from nltk.tokenize import sent_tokenize  # Function for splitting text into sentences using a pre-trained model\n",
        "\n",
        "sentences = sent_tokenize(raw_text)      # Tokenize the document into sentences\n",
        "print(f\"Number of sentences in the document: {len(sentences)}\")\n",
        "print(f\"\\nFirst sentence:\\n{sentences[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the document: 96120\n",
            "First 10 tokens:\n",
            "['Cookie', 'Manager', ':', '``', 'Do', \"n't\", 'allow', 'sites', 'that', 'set']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section shows how to use the `word_tokenize` function to break raw text into individual word tokens (including punctuation).\n",
        "- Highlight the importance of word tokenization as a fundamental step for most NLP tasks, such as stemming, lemmatization,\n",
        "  POS tagging, and text analysis.\n",
        "'''\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "tokens = word_tokenize(raw_text)    # Tokenize the document into words\n",
        "print(f\"Number of tokens in the document: {len(tokens)}\")\n",
        "print(f\"First 10 tokens:\\n{tokens[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px;  color:rgb(38, 255, 18); font-weight: bold\">Semantic, Syntactic, and Sentiment Analysis in NLP</span><br/>\n",
        "▪ `Semantic Analysis:` Interprets the meaning of text in context, enabling computers to understand relationships and resolve ambiguities.<br/>\n",
        "▪ `Syntactic Analysis:` Analyzes sentence structure to identify grammatical relationships between words, such as subjects, verbs, and objects.<br/>\n",
        "▪ `Sentiment Analysis:` Detects the overall emotional tone of text (positive, negative, or neutral), useful for understanding opinions in sources like reviews or social media.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Analysis: Synsets for 'bank':\n",
            "• bank.n.01: sloping land (especially the slope beside a body of water)\n",
            "• depository_financial_institution.n.01: a financial institution that accepts deposits and channels the money into lending activities\n",
            "• bank.n.03: a long ridge or pile\n",
            "• bank.n.04: an arrangement of similar objects in a row or in tiers\n",
            "• bank.n.05: a supply or stock held in reserve for future use (especially in emergencies)\n",
            "• bank.n.06: the funds held by a gambling house or the dealer in some gambling games\n",
            "• bank.n.07: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
            "• savings_bank.n.02: a container (usually with a slot in the top) for keeping money at home\n",
            "• bank.n.09: a building in which the business of banking transacted\n",
            "• bank.n.10: a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
            "• bank.v.01: tip laterally\n",
            "• bank.v.02: enclose with a bank\n",
            "• bank.v.03: do business with a bank or keep an account at a bank\n",
            "• bank.v.04: act as the banker in a game or in gambling\n",
            "• bank.v.05: be in the banking business\n",
            "• deposit.v.02: put into a bank account\n",
            "• bank.v.07: cover with ashes so to control the rate of burning\n",
            "• trust.v.01: have confidence or faith in\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to perform semantic analysis using WordNet, a large lexical database of English.\n",
        "- Show how to use NLTK's WordNet interface to retrieve all possible meanings (synsets) of a given word.\n",
        "- Illustrate how a single word can have multiple senses, each with its own definition.\n",
        "- Provide a preview of the synsets and their definitions for the word \"bank\" to highlight semantic ambiguity and\n",
        "  the richness of lexical resources in NLP.\n",
        "'''\n",
        "\n",
        "from nltk.corpus import wordnet  # Provides access to the WordNet lexical database for lemmatization and semantic analysis\n",
        "\n",
        "word = 'bank'  # Define the word to analyze semantically\n",
        "synsets = wordnet.synsets(word)  # Retrieve all synsets (senses) for the word from WordNet\n",
        "\n",
        "print(f\"Semantic Analysis: Synsets for '{word}':\") \n",
        "for syn in synsets:  # Iterate over each synset for the word\n",
        "    print(f\"• {syn.name()}: {syn.definition()}\")  # Print the synset name and its definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syntactic Analysis: POS Tags for the sentence:\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to perform syntactic analysis using part-of-speech (POS) tagging with NLTK.\n",
        "- Show how to use the `word_tokenize` function to split a sentence into individual word tokens.\n",
        "- Use NLTK's `pos_tag` function to assign grammatical categories (such as noun, verb, adjective, etc.) to each token.\n",
        "- Illustrate how POS tagging helps in understanding the syntactic structure of a sentence, which is essential for parsing,\n",
        "  information extraction, and many downstream NLP tasks.\n",
        "- Provide an example sentence and display the resulting list of (token, POS tag) pairs.\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize   # Function for splitting text into words (and punctuation)\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(sentence)  # Tokenize the sentence into words\n",
        "\n",
        "pos_tags = nltk.pos_tag(tokens)  # Perform part-of-speech tagging on the tokens\n",
        "print(f\"Syntactic Analysis: POS Tags for the sentence:\\n{pos_tags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Analysis: Sentiment scores for the text:\n",
            "{'neg': 0.0, 'neu': 0.221, 'pos': 0.779, 'compound': 0.9336}\n",
            "\n",
            "WordNet Synsets for 'great':\n",
            "[Synset('great.n.01'), Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.13')]\n",
            "\n",
            "Gutenberg Files:\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates three key NLP resources and tasks:\n",
        "- Sentiment Analysis: Uses NLTK's SentimentIntensityAnalyzer to determine the sentiment polarity (positive, negative,\n",
        "  neutral, compound) of a sample text.\n",
        "- WordNet Synsets: Retrieves and displays all synsets (sets of cognitive synonyms) for the word 'great' using WordNet,\n",
        "  illustrating the multiple senses a word can have.\n",
        "- Gutenberg Corpus: Lists the available text files in the NLTK Gutenberg corpus, which is a collection of classic\n",
        "  literature useful for NLP experiments and demonstrations.\n",
        "'''\n",
        "\n",
        "from nltk.corpus import gutenberg  # import the Gutenberg corpus from NLTK\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer  # import the SentimentIntensityAnalyzer for sentiment analysis\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()  # create an instance of SentimentIntensityAnalyzer\n",
        "text = \"I love natural language processing! It's amazing and fun.\"  # sample text for sentiment analysis\n",
        "sentiment_scores = sia.polarity_scores(text)  # get sentiment polarity scores for the text\n",
        "print(f\"Sentiment Analysis: Sentiment scores for the text:\\n{sentiment_scores}\")  # print the sentiment scores\n",
        "\n",
        "# Get synsets (sets of cognitive synonyms) for the word 'great' from WordNet\n",
        "synsets = wordnet.synsets('great')  # retrieve synsets for the word 'great' from WordNet\n",
        "print(f\"\\nWordNet Synsets for 'great':\\n{synsets}\")  # print the synsets for 'great'\n",
        "\n",
        "# list available files in the Gutenberg corpus\n",
        "print(f\"\\nGutenberg Files:\\n{gutenberg.fileids()}\")  # print the list of available files in the Gutenberg corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: rgb(237, 4, 245); font-weight: bold\">Regular Expressions in NLP</span><br/>\n",
        "Regular expressions (regex) are powerful tools used to identify, extract, and manipulate specific patterns within text data.<br/>\n",
        "▪ `Pattern Matching:` Find and match specific sequences of characters (e.g., email addresses, dates, phone numbers) in text.<br/>\n",
        "▪ `Substring Extraction:` Pull out relevant parts of text that match a defined pattern.<br/>\n",
        "▪ `Pattern Replacement/Removal:` Substitute or eliminate text patterns, such as removing unwanted symbols or correcting formats.<br/>\n",
        "▪ `Noise Filtering:` Clean text by filtering out irrelevant or noisy data, making it more suitable for analysis.<br/>\n",
        "\n",
        "<span style=\"font-size: 15.5px; ; font-weight: bold\">Character Ranges and Quantifiers:</span><br/> \n",
        "▪ `[A-Za-z]` matches any uppercase or lowercase letter.<br/> \n",
        "▪ `{2}` means exactly 2 occurrences of the preceding pattern.<br/> \n",
        "▪ `\\d{3}` matches exactly 3 digits (where `\\d` is any digit from 0 to 9).<br/> \n",
        "▪  Square brackets `[]` define a set or range of characters to match.<br/> \n",
        "\n",
        "<span style=\"font-size: 15.5px; ; font-weight: bold\">Example:</span><br/> \n",
        "^[\\w\\.-]+@([\\w-]+\\.)+[\\w-]{2,4}\n",
        "\n",
        "`^[\\w\\.-]+`: Matches the start of the string and then one or more word characters (`\\w`), dots (`.`), or hyphens (`-`). This is the username part of the email.<br/>\n",
        "`@` : Matches the literal '@' symbol.<br/> \n",
        "`([\\w-]+\\.)+` : Matches one or more groups of word characters or hyphens followed by a dot. This covers subdomains and the main domain.<br/> \n",
        "`[\\w-]{2,4}$` : Matches the top-level domain (TLD) at the end, which must be 2 to 4 word characters or hyphens.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Regex Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '123', '252', 'times']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to use regular expressions (regex) in NLP for pattern-based text processing. \n",
        "- It introduces the RegexpTokenizer from NLTK, which allows tokenization of text based on custom regex patterns,\n",
        "  enabling extraction of specific types of tokens (e.g., words, numbers).\n",
        "'''\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer  # Import RegexpTokenizer from NLTK\n",
        "\n",
        "example_text = \"The quick brown fox jumps over the lazy dog, 123-252 times!\"  # Example text to tokenize\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # Create a tokenizer that matches words (alphanumeric sequences)\n",
        "tokens = tokenizer.tokenize(example_text)  # Tokenize the example text using the regex tokenizer\n",
        "print(\"NLTK Regex Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected Emails: ['admin.support_34@example.com', 'sales-dep@company.org']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to use Python's built-in `re` module to extract email addresses from text using regular expressions.\n",
        "- The `re.findall()` function is used to search the text and return all substrings that match the email pattern.\n",
        "- This is useful in NLP tasks for extracting contact information, cleaning data, or identifying entities in unstructured text.\n",
        "'''\n",
        "\n",
        "import re  # Import the regular expressions module\n",
        "text_emails = \"Contact us at admin.support_34@example.com or sales-dep@company.org for inquiries.\"  # Example text containing email addresses\n",
        "\n",
        "# Define a regular expression pattern to match email addresses:\n",
        "# [a-zA-Z0-9._%+-]+   : Matches one or more allowed characters in the username part (letters, digits, dot, underscore, percent, plus, hyphen)\n",
        "# @                   : Matches the '@' symbol\n",
        "# [a-zA-Z0-9.-]+      : Matches one or more allowed characters in the domain name (letters, digits, dot, hyphen)\n",
        "# \\.                  : Matches a literal dot before the domain extension\n",
        "# [a-zA-Z]{2,}        : Matches the domain extension (at least two letters)\n",
        "email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "\n",
        "emails = re.findall(email_pattern, text_emails)  # Find all email addresses in the text using the regex pattern\n",
        "print(\"Detected Emails:\", emails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capitalized words found in the sentence:\n",
            "• Alice\n",
            "• Bob\n",
            "• New\n",
            "• York\n",
            "• City\n",
            "• Friday\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to use spaCy's Matcher for advanced pattern-based token matching in NLP.\n",
        "- The Matcher allows you to define custom token patterns (using dictionaries) and search for them in a processed document.\n",
        "- Here, we define a pattern to match words that start with a capital letter (e.g., proper nouns, names, places).\n",
        "- This is useful for extracting named entities, titles, or any tokens that follow a specific capitalization rule.\n",
        "'''\n",
        "\n",
        "import spacy  # Import the spaCy library for advanced NLP tasks\n",
        "from spacy.matcher import Matcher  # Import the Matcher class for pattern matching in spaCy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model\n",
        "matcher = Matcher(nlp.vocab)        # Create a Matcher object\n",
        "\n",
        "# Define a pattern to match words that start with a capital letter\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}}]\n",
        "matcher.add(\"CAPITALIZED_WORD\", [pattern])\n",
        "\n",
        "text = \"Alice and Bob went to New York City last Friday.\" # Example sentence\n",
        "doc = nlp(text)  # Process the text\n",
        "matches = matcher(doc)  # Apply the matcher to the doc\n",
        "\n",
        "# Print the matched tokens\n",
        "print(\"Capitalized words found in the sentence:\")\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(\"•\", span.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font*size: 16px; color: rgb(6, 168, 243); font-weight: bold\">Stopwords Removal</span><br/>\n",
        "Stopwords are common words (such as \"the\", \"is\", \"in\", \"and\") that are often removed from text before processing, as they typically do not carry significant meaning. Filtering out stopwords helps reduce noise and improves the efficiency of text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Words: ['example', 'sentence', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to remove stopwords from a text using NLTK.\n",
        "- The code tokenizes the input text into words and then filters out any word that appears in NLTK's list of English stopwords.\n",
        "- The result is a list of words from the original text with stopwords removed, which can help improve the quality of text analysis.\n",
        "'''\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"This is an example sentence showing off the stop words filtration.\"\n",
        "words = nltk.word_tokenize(text)  # tokenize the text into words using nltk\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]  # filter out stopwords\n",
        "print(\"Filtered Words:\", filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "Alice and Bob went to New York City last Friday.\n",
            "\n",
            "After Stopwords Removal:\n",
            "Alice Bob went New York City Friday .\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to remove stopwords from a text using spaCy.\n",
        "- The code processes the input text with spaCy's language model to tokenize it.\n",
        "- It then filters out tokens that are identified as stopwords by spaCy (using the `is_stop` attribute).\n",
        "- The result is a list of tokens from the original text with stopwords removed, which can help improve the quality of text analysis.\n",
        "'''\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model\n",
        "doc = nlp(text)  # Process the text with spaCy\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]  # Create a list of tokens that are not stopwords\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nAfter Stopwords Removal:\")  # Print the label for filtered text\n",
        "print(\" \".join(filtered_tokens))  # Print the filtered text without stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: rgb(12, 238, 125); font-weight: bold\">Part-of-Speech (POS) Tagging in NLP</span><br/>\n",
        "POS tagging is the process of labeling each word in a sentence with its corresponding part of speech, such as `noun`, `verb`, `adjective`, etc. This helps computers understand the grammatical structure and meaning of text.<br/>\n",
        "\n",
        "<span style=\"font-size: 15.5px; font-weight: bold\">Common POS Tags and Examples:</span><br/>\n",
        "▪ `Noun (N):` Names of people, places, things, or ideas.<br/>\n",
        "▪ `Verb (V):` Words that express actions or states.<br/>\n",
        "▪ `Adjective (ADJ):` Words that describe nouns.<br/>\n",
        "▪ `Adverb (ADV):` Words that modify verbs, adjectives, or other adverbs.<br/>\n",
        "▪ `Preposition (P):` Words that show relationships between nouns or pronouns and other words (at, on, in, from, with, near, between, about, under).<br/>\n",
        "▪ `Conjunction (CON):` Words that connect clauses, sentences, or words (and, or, but, because, so, yet, unless, since, if).<br/>\n",
        "▪ `Pronoun (PRO):` Words that replace nouns (you, I, we, they, he, she, it, me, us, them, him, her, this).<br/>\n",
        "▪ `Interjection (INT):` Words or phrases that express emotion or exclamation (Ouch! Wow! Great! Help! Oh! Hey! Hi!).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Tags using NLTK:/n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "\n",
            "POS Tags using spaCy:[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to perform Part-of-Speech (POS) tagging using both NLTK and spaCy.\n",
        "- NLTK's `pos_tag` function is used after tokenizing the sentence to assign POS tags.\n",
        "- spaCy's language model is also used to process the sentence and extract POS tags for each token.\n",
        "'''\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence) # Tokenize the sentence\n",
        "pos_tags = nltk.pos_tag(tokens) # POS tagging using NLTK\n",
        "print(f\"POS Tags using NLTK:\\n{pos_tags}\")\n",
        "\n",
        "doc = nlp(sentence) # POS tagging using spaCy\n",
        "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(f\"\\nPOS Tags using spaCy:{spacy_pos_tags}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: rgb(171, 12, 245); font-weight: bold\">Chunking and Parsing in NLP</span><br/>\n",
        "Chunking and parsing are essential techniques in NLP for understanding the structure and meaning of sentences.<br/>\n",
        "▪ `Group words into meaningful chunks:` Chunking segments sentences into groups of words (such as noun or verb phrases) that function together as a unit.<br/>\n",
        "▪ `Identify phrase boundaries:` These methods help determine where phrases begin and end, making it easier to extract relevant information.<br/>\n",
        "▪ `Analyze grammatical structure:` Parsing examines the grammatical relationships between words and phrases, revealing how sentences are constructed.<br/>\n",
        "▪ `Reveal sentence hierarchy:` Parsing uncovers the hierarchical structure of a sentence, showing how smaller chunks combine to form larger grammatical units.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunked Sentence Structure:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n",
            "                                S                                          \n",
            "     ___________________________|_______________________________            \n",
            "    |        |     |            NP               NP             NP         \n",
            "    |        |     |     _______|________        |       _______|______     \n",
            "jumps/VBZ over/IN ./. The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to perform chunking (shallow parsing) using NLTK.\n",
        "- Tokenize and POS tag a sample sentence.\n",
        "- Define a chunk grammar to identify noun phrases (NP) using regular expressions.\n",
        "- Use NLTK's RegexpParser to parse the POS-tagged sentence and extract the chunks.\n",
        "'''\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize and POS tag the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define a chunk grammar for noun phrases (NP)\n",
        "# The pattern \"NP: {<DT>?<JJ>*<NN>}\" can be broken down as follows:\n",
        "# - NP: This is the label for the chunk (Noun Phrase).\n",
        "# - { ... }: The curly braces enclose the pattern to match.\n",
        "# - <DT>? : An optional determiner (e.g., \"the\", \"a\"). The question mark means it may appear 0 or 1 time.\n",
        "# - <JJ>* : Zero or more adjectives (e.g., \"quick\", \"brown\"). The asterisk means any number of adjectives can appear.\n",
        "# - <NN>  : A singular noun (e.g., \"fox\", \"dog\"). This is required at the end of the pattern.\n",
        "# This grammar will match sequences like \"the quick brown fox\" or \"lazy dog\" as noun phrases.\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)   # Create a RegexpParser object and parse the tagged tokens\n",
        "\n",
        "# Parse the POS-tagged sentence to get chunks\n",
        "tree = cp.parse(pos_tags)\n",
        "\n",
        "# Print the chunked structure\n",
        "print(\"Chunked Sentence Structure:\")\n",
        "print(tree)\n",
        "\n",
        "tree.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Noun Chunks List:\n",
            "['The quick brown fox', 'the lazy dog']\n",
            "\n",
            "NLTK Noun Chunks List:\n",
            "['The quick brown', 'fox', 'the lazy dog']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section compares noun phrase (noun chunk) extraction using two popular NLP libraries: spaCy and NLTK.\n",
        "- First, we use spaCy's built-in noun_chunks attribute to extract noun phrases directly from the processed sentence.\n",
        "- Then, we use the chunked parse tree generated by NLTK's RegexpParser (from the previous section) to extract noun\n",
        "  phrases labeled as 'NP'.\n",
        "'''\n",
        "\n",
        "sen = nlp(sentence)  # Process the sentence using spaCy's NLP pipeline\n",
        "spacy_noun_chunks = [chunk.text for chunk in sen.noun_chunks]  # Extract noun chunks from the processed sentence using spaCy\n",
        "print(f\"spaCy Noun Chunks List:\\n{spacy_noun_chunks}\")\n",
        "\n",
        "# Create a list of noun chunks using NLTK (from the previous chunked tree)\n",
        "nltk_noun_chunks = []  # Initialize an empty list to store NLTK noun chunks\n",
        "for subtree in tree.subtrees():  # Iterate over all subtrees in the chunked tree\n",
        "    if subtree.label() == 'NP':  # Check if the subtree is labeled as a noun phrase (NP)\n",
        "        chunk = \" \".join(word for word, pos in subtree.leaves())  # Join the words in the noun phrase chunk\n",
        "        nltk_noun_chunks.append(chunk)  # Add the noun phrase chunk to the list\n",
        "print(f\"\\nNLTK Noun Chunks List:\\n{nltk_noun_chunks}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: #eb5e28; font-weight: bold\">Hypernyms and Hyponyms in NLP</span><br/>\n",
        "▪ `Hypernyms` are words that denote a broad category or general class (e.g., \"animal\" is a hypernym of \"dog\").<br/>\n",
        "▪ `Hyponyms` are words that represent a more specific instance within a category (e.g., \"poodle\" is a hyponym of \"dog\").<br/>\n",
        "Recognizing hypernyms and hyponyms helps build a *semantic hierarchy*, allowing NLP systems to understand relationships between general and specific terms.<br/>\n",
        "This enhances *lexical organization* and improves tasks like information retrieval, question answering, and semantic search by enabling systems to group, relate, and infer meaning from word relationships.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset for 'dog': dog.n.01 - a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
            "\n",
            "Hypernyms:\n",
            "• canine.n.02 - any of various fissiped mammals with nonretractile claws and typically long muzzles\n",
            "• domestic_animal.n.01 - any of various animals that have been tamed and made fit for a human environment\n",
            "\n",
            "Hyponyms:\n",
            "• basenji.n.01 - small smooth-haired breed of African origin having a tightly curled tail and the inability to bark\n",
            "• corgi.n.01 - either of two Welsh breeds of long-bodied short-legged dogs with erect ears and a fox-like head\n",
            "• cur.n.01 - an inferior dog or one of mixed breed\n",
            "• dalmatian.n.02 - a large breed having a smooth white coat with black or brown spots; originated in Dalmatia\n",
            "• great_pyrenees.n.01 - bred of large heavy-coated white dogs resembling the Newfoundland\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to use WordNet (via NLTK) to explore semantic relationships for a given word, specifically\n",
        "hypernyms and hyponyms.\n",
        "- We look up the synsets (word senses) for a noun (here, \"dog\").\n",
        "'''\n",
        "\n",
        "word = \"dog\"  # Define the word to look up in WordNet\n",
        "synsets = wordnet.synsets(word, pos=wordnet.NOUN)  # Get all noun synsets for the word\n",
        "\n",
        "if synsets:  # Check if any synsets were found\n",
        "    syn = synsets[0]  # Take the first synset as an example\n",
        "    print(f\"Synset for '{word}': {syn.name()} - {syn.definition()}\")  # Print the synset name and definition\n",
        "    \n",
        "    # Get hypernyms (more general terms)\n",
        "    hypernyms = syn.hypernyms()  # Retrieve hypernyms for the synset\n",
        "    print(\"\\nHypernyms:\")\n",
        "    for h in hypernyms:  # Iterate over each hypernym\n",
        "        print(f\"• {h.name()} - {h.definition()}\")  # Print the hypernym name and definition\n",
        "    \n",
        "    # Get hyponyms (more specific terms)\n",
        "    hyponyms = syn.hyponyms()  # Retrieve hyponyms for the synset\n",
        "    print(\"\\nHyponyms:\")\n",
        "    for h in hyponyms[:5]:  # Show only first 5 hyponyms for brevity\n",
        "        print(f\"• {h.name()} - {h.definition()}\")  # Print the hyponym name and definition\n",
        "else:  # If no synsets were found\n",
        "    print(f\"No synsets found for '{word}'.\")  # Print a message indicating no synsets found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: #cbaf89; font-weight: bold\">Named Entity Recognition (NER)</span><br/>\n",
        "NER is a fundamental task in NLP that involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, dates, and more.<br/>\n",
        "NER helps extract structured information from unstructured text, enabling applications like information extraction, question answering, and knowledge graph construction.<br/>\n",
        "Modern NLP libraries like spaCy and NLTK provide built-in tools for performing NER efficiently.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy NER Results:\n",
            "• Barack Obama (PERSON)\n",
            "• Hawaii (GPE)\n",
            "• 44th (ORDINAL)\n",
            "• the United States (GPE)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to perform Named Entity Recognition (NER) using spaCy.\n",
        "- The text is processed to create a Doc object, which contains detected entities.\n",
        "- We then iterate through the recognized entities and print each entity's text along with its label (e.g., PERSON, GPE, ORG).\n",
        "'''\n",
        "\n",
        "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy NER Results:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"• {ent.text} ({ent.label_})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px;font-weight:bold; color: rgb(248, 17, 28)\"> Stemming & Lemmatization</span><br/>\n",
        "Stemming and Lemmatization are two fundamental techniques in NLP used to reduce words to their root or base forms.<br/>\n",
        "\n",
        "**Stemming:**<br/>\n",
        "▪ Stemming is the process of removing suffixes (and sometimes prefixes) from words to obtain their stem or root form.<br/>\n",
        "▪ The resulting stem may not always be a valid word in the language, but it helps group together words with similar meanings (e.g., \"playing\", \"played\", \"plays\" → \"play\").<br/>\n",
        "▪ Stemming algorithms are typically rule-based and fast, but can be less accurate.<br/>\n",
        "\n",
        "**Lemmatization:**<br/>\n",
        "▪ Lemmatization reduces words to their base or dictionary form, known as the lemma.<br/>\n",
        "▪ Unlike stemming, lemmatization considers the context and part of speech of a word, ensuring that the root form is a valid word (e.g., \"better\" → \"good\", \"running\" → \"run\").<br/>\n",
        "▪ Lemmatization is generally more accurate but may require more computational resources and linguistic knowledge.<br/>\n",
        "\n",
        "**Workflow for Stemming and Lemmatization:**<br/>\n",
        "▪ `Lowercasing:` Convert all text to lowercase for consistency.<br/>\n",
        "▪ `Tokenization:` Split text into individual words (tokens).<br/>\n",
        "▪ `Stemming/Lemmatization:` Apply stemming or lemmatization to each token to obtain root forms.<br/>\n",
        "▪ `Reconstruction (Optional):` Reconstruct the processed tokens back into text for further analysis.<br/>\n",
        "\n",
        "These techniques are commonly used in text preprocessing to normalize words, improve search results, and enhance the performance of NLP models.<br/>\n",
        "\n",
        "**Difference between Stemming and Lemmatization (with Lancaster Stemmer):**<br/>\n",
        "The main difference between stemming and lemmatization is that stemming crudely removes word suffixes to arrive at a root form, which may not be a valid word, while lemmatization reduces words to their dictionary form (lemma), considering context and part of speech.<br/>\n",
        "\n",
        "**Stemming (Lancaster):**<br/>\n",
        "▪ The Lancaster stemmer is more aggressive than the Porter stemmer, often producing shorter stems.<br/>\n",
        "▪ Example: \"playing\", \"played\", \"plays\" → \"play\" (Porter), but Lancaster may reduce further.<br/>\n",
        "\n",
        "**Lemmatization:**<br/>\n",
        "▪ Lemmatization always returns a valid word (lemma) and is context-aware.<br/>\n",
        "▪ Example: \"better\" → \"good\" (with POS), \"running\" → \"run\".<br/>\n",
        "\n",
        "**Comparison Table:**<br/>\n",
        "| Word      | Lancaster Stem | Porter Stem | Lemma      |\n",
        "|-----------|:--------------|:------------|:-----------|\n",
        "| playing   | play           | play        | playing    |\n",
        "| played    | play           | play        | played     |\n",
        "| plays     | play           | play        | play       |\n",
        "| better    | bet            | better      | better     |\n",
        "| running   | run            | run         | running    |\n",
        "\n",
        "The Lancaster stemmer can be too aggressive for some applications, while lemmatization is more accurate but slower.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>tokens</th>\n",
              "      <th>stems</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cats are running</td>\n",
              "      <td>[cats, are, running]</td>\n",
              "      <td>[cat, are, run]</td>\n",
              "      <td>[cat, are, running]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dogs played outside</td>\n",
              "      <td>[dogs, played, outside]</td>\n",
              "      <td>[dog, play, outsid]</td>\n",
              "      <td>[dog, played, outside]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              original                   tokens                stems  \\\n",
              "0     Cats are running     [cats, are, running]      [cat, are, run]   \n",
              "1  Dogs played outside  [dogs, played, outside]  [dog, play, outsid]   \n",
              "\n",
              "                   lemmas  \n",
              "0     [cat, are, running]  \n",
              "1  [dog, played, outside]  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to perform stemming and lemmatization on a set of example sentences using NLTK's \n",
        "  PorterStemmer and WordNetLemmatizer. \n",
        "- Stemming reduces words to their root form by removing suffixes, which may not always result in a valid word. \n",
        "- Lemmatization, on the other hand, reduces words to their base or dictionary form (lemma), ensuring the result is a valid word. \n",
        "- The code tokenizes each sentence, applies stemming and lemmatization to each token, and then stores the results \n",
        "  in a pandas DataFrame for easy comparison.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "documents = [\n",
        "    \"Cats are running\",\n",
        "    \"Dogs played outside\",\n",
        "]\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()  # Create a stemmer object using the Porter algorithm\n",
        "lemmatizer = WordNetLemmatizer() # Create a lemmatizer object using WordNet\n",
        "\n",
        "# Tokenize, Stem, and Lemmatize each document\n",
        "results = []  # Initialize an empty list to store results for each document\n",
        "for doc in documents:  # Iterate over each document in the documents list\n",
        "    tokens = word_tokenize(doc.lower())  # Tokenize the document after converting it to lowercase\n",
        "    stems = [stemmer.stem(token) for token in tokens]  # Apply stemming to each token\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]  # Apply lemmatization to each token\n",
        "    results.append({  # Append a dictionary with original text, tokens, stems, and lemmas to the results list\n",
        "        \"original\": doc,  # Store the original document text\n",
        "        \"tokens\": tokens,  # Store the list of tokens\n",
        "        \"stems\": stems,  # Store the list of stemmed tokens\n",
        "        \"lemmas\": lemmas  # Store the list of lemmatized tokens\n",
        "    })\n",
        "\n",
        "# Display results in a DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stemming vs Lemmatization Comparison:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>playing</td>\n",
              "      <td>play</td>\n",
              "      <td>playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>played</td>\n",
              "      <td>play</td>\n",
              "      <td>played</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plays</td>\n",
              "      <td>play</td>\n",
              "      <td>play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>running</td>\n",
              "      <td>run</td>\n",
              "      <td>running</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word    stem    lemma\n",
              "0  playing    play  playing\n",
              "1   played    play   played\n",
              "2    plays    play     play\n",
              "3   better  better   better\n",
              "4  running     run  running"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "- This section compares the results of stemming and lemmatization for a small set of sample words using NLTK's \n",
        "  PorterStemmer and WordNetLemmatizer. \n",
        "- It demonstrates how stemming may produce non-standard word forms, while lemmatization aims to return valid dictionary words. \n",
        "'''\n",
        "# Compare stemming and lemmatization for a few words\n",
        "sample_words = [\"playing\", \"played\", \"plays\", \"better\", \"running\", \"feet\"]\n",
        "comparison = []\n",
        "for word in sample_words:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    comparison.append({\"word\": word, \"stem\": stem, \"lemma\": lemma})\n",
        "\n",
        "print(\"\\nStemming vs Lemmatization Comparison:\")\n",
        "df = pd.DataFrame(comparison)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stemming vs Lemmatization Comparison:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>playing</td>\n",
              "      <td>play</td>\n",
              "      <td>playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>played</td>\n",
              "      <td>play</td>\n",
              "      <td>played</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plays</td>\n",
              "      <td>play</td>\n",
              "      <td>play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>running</td>\n",
              "      <td>run</td>\n",
              "      <td>running</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word    stem    lemma\n",
              "0  playing    play  playing\n",
              "1   played    play   played\n",
              "2    plays    play     play\n",
              "3   better  better   better\n",
              "4  running     run  running"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates the difference between stemming and lemmatization using NLTK's PorterStemmer and WordNetLemmatizer. \n",
        "- By comparing the outputs for a set of sample words, you can observe how each technique processes the words differently.\n",
        "'''\n",
        "# Example: Compare stemming and lemmatization for a few words\n",
        "sample_words = [\"playing\", \"played\", \"plays\", \"better\", \"running\", \"feet\"]\n",
        "comparison = []\n",
        "for word in sample_words:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    comparison.append({\"word\": word, \"stem\": stem, \"lemma\": lemma})\n",
        "\n",
        "print(\"\\nStemming vs Lemmatization Comparison:\")\n",
        "df = pd.DataFrame(comparison)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "spaCy Lemmatization and POS Tagging:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>tokens</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cats are running</td>\n",
              "      <td>[Cats, are, running]</td>\n",
              "      <td>[cat, be, run]</td>\n",
              "      <td>[NOUN, AUX, VERB]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dogs played outside</td>\n",
              "      <td>[Dogs, played, outside]</td>\n",
              "      <td>[dog, play, outside]</td>\n",
              "      <td>[NOUN, VERB, ADV]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              original                   tokens                lemmas  \\\n",
              "0     Cats are running     [Cats, are, running]        [cat, be, run]   \n",
              "1  Dogs played outside  [Dogs, played, outside]  [dog, play, outside]   \n",
              "\n",
              "                 pos  \n",
              "0  [NOUN, AUX, VERB]  \n",
              "1  [NOUN, VERB, ADV]  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates how to use spaCy to perform lemmatization and part-of-speech (POS) tagging on example sentences. \n",
        "- Lemmatization reduces words to their base or dictionary form, while POS tagging assigns grammatical categories (such as \n",
        "  NOUN, VERB, etc.) to each token. \n",
        "- The code processes each sentence, extracts the original tokens, their lemmas, and POS tags, and then displays the\n",
        "results in a pandas DataFrame for easy comparison.\n",
        "'''\n",
        "\n",
        "# Load the small English model in spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentences\n",
        "spacy_docs = [\n",
        "    \"Cats are running\",\n",
        "    \"Dogs played outside\",\n",
        "]\n",
        "\n",
        "# Process each document with spaCy\n",
        "spacy_results = []\n",
        "for doc in spacy_docs:\n",
        "    spacy_doc = nlp(doc)\n",
        "    tokens = [token.text for token in spacy_doc]\n",
        "    lemmas = [token.lemma_ for token in spacy_doc]\n",
        "    pos = [token.pos_ for token in spacy_doc]\n",
        "    spacy_results.append({\n",
        "        \"original\": doc,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas,\n",
        "        \"pos\": pos\n",
        "    })\n",
        "\n",
        "# Display results in a DataFrame\n",
        "spacy_df = pd.DataFrame(spacy_results)\n",
        "print(\"\\nspaCy Lemmatization and POS Tagging:\")\n",
        "spacy_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:#d93f26; font-size: 16.5px; font-weight: bold\">Tokenization Concepts</span><br/>\n",
        "Tokenization is the process of breaking down text into smaller units called tokens. It's a fundamental step in NLP that helps computers understand and process human language by converting text into a format they can work with.<br/>\n",
        "\n",
        "<span style=\"font-size: 16.5px; font-weight: bold\">Types of tokenization:</span><br/>\n",
        "▪ `Sentence Tokenization:` Splits text into individual sentences, useful for document-level analysis<br/>\n",
        "▪ `Word Tokenization:` Breaks text into individual words, essential for word-level processing<br/>\n",
        "▪ `Regex Tokenization:` Uses regular expressions to extract specific patterns from text<br/>\n",
        "▪ `Treebank Tokenization:` Follows Penn Treebank conventions for standardized word tokenization<br/>\n",
        "▪ `WordPunct Tokenization:` Separates words and punctuation into distinct tokens<br/>\n",
        "▪ `Whitespace Tokenization:` Splits text based on spaces, the simplest form of tokenization<br/>\n",
        "▪ `Character Tokenization:` Breaks text into individual characters, useful for character-level analysis<br/>\n",
        "\n",
        "The [Webtext Corpus](https://paperswithcode.com/dataset/webtext) is a high-quality dataset that can be used to train custom tokenizers. It contains diverse text samples that help create robust tokenization models capable of handling various text patterns and formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence tokenization:\n",
            "['I am learning Natural Language Processing.', \"I'm learning Python programming.\", 'It is very user friendly.', \"I'm ready to start coding.\"]\n",
            "\n",
            "Word tokenization:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.', 'I', \"'m\", 'learning', 'Python', 'programming', '.', 'It', 'is', 'very', 'user', 'friendly', '.', 'I', \"'m\", 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "Regex tokenization (words only):\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', 'I', 'm', 'learning', 'Python', 'programming', 'It', 'is', 'very', 'user', 'friendly', 'I', 'm', 'ready', 'to', 'start', 'coding']\n",
            "\n",
            "TreebankWordTokenizer:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing.', 'I', \"'m\", 'learning', 'Python', 'programming.', 'It', 'is', 'very', 'user', 'friendly.', 'I', \"'m\", 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "WordPunctTokenizer:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.', 'I', \"'\", 'm', 'learning', 'Python', 'programming', '.', 'It', 'is', 'very', 'user', 'friendly', '.', 'I', \"'\", 'm', 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "Whitespace tokenization:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing.', \"I'm\", 'learning', 'Python', 'programming.', 'It', 'is', 'very', 'user', 'friendly.', \"I'm\", 'ready', 'to', 'start', 'coding.']\n",
            "\n",
            "Character tokenization:\n",
            "['I', ' ', 'a', 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '.', ' ', 'I', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'u', 's', 'e', 'r', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 'l', 'y', '.', ' ', 'I', \"'\", 'm', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 's', 't', 'a', 'r', 't', ' ', 'c', 'o', 'd', 'i', 'n', 'g', '.']\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- This section demonstrates various tokenization techniques in NLP using NLTK. \n",
        "- Different tokenizers serve different purposes, such as splitting text into sentences, extracting words while removing\n",
        "punctuation, or preserving punctuation as separate tokens. \n",
        "'''\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer     # Class that splits text into word and punctuation tokens separately\n",
        "\n",
        "txt = \"I am learning Natural Language Processing. I'm learning Python programming. It is very user friendly. I'm ready to start coding.\"\n",
        "\n",
        "# Using sent_tokenize to split text into sentences\n",
        "# This is useful when you need to process text at the sentence level\n",
        "sent_tok = sent_tokenize(txt)\n",
        "print(f\"Sentence tokenization:\\n{sent_tok}\")\n",
        "\n",
        "# Using word_tokenize to split text into individual words\n",
        "# This is useful for word-level analysis and processing\n",
        "word_tok = word_tokenize(txt)\n",
        "print(f\"\\nWord tokenization:\\n{word_tok}\")\n",
        "\n",
        "# Using RegexpTokenizer to extract only word characters\n",
        "# This is useful when you want to remove punctuation and keep only alphanumeric characters\n",
        "# The pattern r\"\\w+\" matches sequences of alphanumeric characters (letters, digits, and underscores).\n",
        "# It is used here to extract only \"word\" tokens, ignoring punctuation and spaces.\n",
        "tok = RegexpTokenizer(r\"\\w+\")\n",
        "print(f\"\\nRegex tokenization (words only):\\n{tok.tokenize(txt)}\")\n",
        "\n",
        "# Using TreebankWordTokenizer for standard word tokenization\n",
        "# This follows the Penn Treebank tokenization conventions\n",
        "tree_tok = nltk.TreebankWordTokenizer()\n",
        "print(f\"\\nTreebankWordTokenizer:\\n{tree_tok.tokenize(txt)}\")\n",
        "\n",
        "# Using WordPunctTokenizer to split text into words and punctuation\n",
        "# This is useful when you need to preserve punctuation as separate tokens\n",
        "punkt_tok = WordPunctTokenizer()\n",
        "print(f\"\\nWordPunctTokenizer:\\n{punkt_tok.tokenize(txt)}\")\n",
        "\n",
        "# Using simple whitespace tokenization\n",
        "# This is the most basic form of tokenization, splitting on spaces\n",
        "print(f\"\\nWhitespace tokenization:\\n{txt.split()}\")\n",
        "\n",
        "# Using character-level tokenization\n",
        "# This is useful for character-level analysis or when working with non-standard text\n",
        "print(f\"\\nCharacter tokenization:\\n{list(txt)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16.5px; font-weight: bold; color:#80ed99\">Custom Tokenizer Training</span><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Mr reza. How are you today? I can't stand this weather.\n",
            "The sun is too bright and the temperature is unbearable.\n",
            "I don't know how people can work in these conditions.\n",
            "Maybe we should move to a cooler place.\n",
            "What do you think about that?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Hello!',\n",
              " 'Mr reza.',\n",
              " 'How are you today?',\n",
              " \"I can't stand this weather.\",\n",
              " 'The sun is too bright and the temperature is unbearable.',\n",
              " \"I don't know how people can work in these conditions.\",\n",
              " 'Maybe we should move to a cooler place.',\n",
              " 'What do you think about that?']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to use a custom-trained sentence tokenizer in NLTK.\n",
        "- It loads the pre-trained English Punkt tokenizer model, which is an unsupervised algorithm for sentence boundary detection.\n",
        "- It then reads a sample text file from disk.\n",
        "- Finally, it uses the loaded Punkt tokenizer to split the text into sentences.\n",
        "This approach is useful when you want more accurate sentence tokenization, especially for text with non-standard \n",
        "sentence boundaries or punctuation.\n",
        "'''\n",
        "\n",
        "import nltk.data                 # Used for loading NLTK resources and models\n",
        "\n",
        "# Load the pre-trained English Punkt tokenizer model\n",
        "punkt_tok = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Open a text file using the correct relative path (adjusted for your project structure)\n",
        "txt_file = open(\"D:/Natural-language-processing/Data/sample_text.txt\", mode='r', encoding='utf-8')\n",
        "\n",
        "txt_read = txt_file.read()\n",
        "print(txt_read)\n",
        "\n",
        "# Tokenize the text using the loaded Punkt tokenizer\n",
        "tok = punkt_tok.tokenize(txt_read)\n",
        "tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw text data from the 'overheard.txt' file in the webtext corpus\n",
        "text_parameter = webtext.raw('overheard.txt')\n",
        "# print(text_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nltk.tokenize.punkt.PunktSentenceTokenizer"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to train a custom sentence tokenizer using NLTK's PunktSentenceTokenizer.\n",
        "- PunktSentenceTokenizer is an unsupervised machine learning algorithm for sentence boundary detection.\n",
        "- By training it on our own text (in this case, the 'overheard.txt' file from the webtext corpus), the tokenizer\n",
        "  learns the specific sentence boundary conventions, abbreviations, and writing style present in that dataset.\n",
        "- This can improve sentence tokenization accuracy for domain-specific or non-standard text, compared to using the\n",
        "  default pre-trained model.\n",
        "'''\n",
        "\n",
        "from nltk.tokenize import PunktSentenceTokenizer  # Class for sentence tokenization, can be trained on custom data for better accuracy\n",
        "\n",
        "# Train the tokenizer on the specific writing style, abbreviations, and sentence boundaries present in 'overheard.txt'\n",
        "my_tok = PunktSentenceTokenizer(text_parameter)\n",
        "type(my_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pre_token[0]: White guy: So, do you have any plans for this evening?\n",
            "our_token[0]: White guy: So, do you have any plans for this evening?\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This section compares sentence tokenization using two different approaches:\n",
        "- The first approach uses NLTK's built-in `sent_tokenize` function, which relies on a pre-trained Punkt sentence\n",
        "  tokenizer for English. This is a general-purpose model trained on a large, diverse corpus.\n",
        "- The second approach uses a custom-trained `PunktSentenceTokenizer` (created in the previous cell as `my_tok`), \n",
        "   which has been specifically trained on the 'overheard.txt' dataset from the webtext corpus. This allows the \n",
        "   tokenizer to adapt to the unique sentence boundary conventions and writing style of that dataset.\n",
        "\n",
        "By printing the first sentence tokenized by each method, we can observe any differences in how the two tokenizers segment the text.\n",
        "'''\n",
        "\n",
        "# Tokenize the text using the pre-trained sent_tokenize function\n",
        "pre_token = sent_tokenize(text_parameter)\n",
        "\n",
        "# Tokenize the text using our custom trained tokenizer\n",
        "our_token = my_tok.tokenize(text_parameter)\n",
        "\n",
        "print(f\"pre_token[0]: {pre_token[0]}\")\n",
        "\n",
        "print(f\"our_token[0]: {our_token[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16.5px; font-weight: bold; color:#368f8b\">SpaCy Linguistic Analysis</span><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>Lemma</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Dep</th>\n",
              "      <th>Shape</th>\n",
              "      <th>Is alpha</th>\n",
              "      <th>Is stop</th>\n",
              "      <th>Is punctuation</th>\n",
              "      <th>Head</th>\n",
              "      <th>Children</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Apple</td>\n",
              "      <td>Apple</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>be</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>AUX</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>aux</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>looking</td>\n",
              "      <td>look</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBG</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[Apple, is, at, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>ADP</td>\n",
              "      <td>IN</td>\n",
              "      <td>prep</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[buying]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buying</td>\n",
              "      <td>buy</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBG</td>\n",
              "      <td>pcomp</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>at</td>\n",
              "      <td>[startup]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U.K.</td>\n",
              "      <td>U.K.</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>X.X.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>startup</td>\n",
              "      <td>startup</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBD</td>\n",
              "      <td>ccomp</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>buying</td>\n",
              "      <td>[U.K., for]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>for</td>\n",
              "      <td>for</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>ADP</td>\n",
              "      <td>IN</td>\n",
              "      <td>prep</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>[billion]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$</td>\n",
              "      <td>$</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>SYM</td>\n",
              "      <td>$</td>\n",
              "      <td>quantmod</td>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>NUM</td>\n",
              "      <td>CD</td>\n",
              "      <td>compound</td>\n",
              "      <td>d</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>billion</td>\n",
              "      <td>billion</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>NUM</td>\n",
              "      <td>CD</td>\n",
              "      <td>pobj</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>for</td>\n",
              "      <td>[$, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>looking</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Token    Lemma                                           Sentence  \\\n",
              "0     Apple    Apple  Apple is looking at buying U.K. startup for $1...   \n",
              "1        is       be  Apple is looking at buying U.K. startup for $1...   \n",
              "2   looking     look  Apple is looking at buying U.K. startup for $1...   \n",
              "3        at       at  Apple is looking at buying U.K. startup for $1...   \n",
              "4    buying      buy  Apple is looking at buying U.K. startup for $1...   \n",
              "5      U.K.     U.K.  Apple is looking at buying U.K. startup for $1...   \n",
              "6   startup  startup  Apple is looking at buying U.K. startup for $1...   \n",
              "7       for      for  Apple is looking at buying U.K. startup for $1...   \n",
              "8         $        $  Apple is looking at buying U.K. startup for $1...   \n",
              "9         1        1  Apple is looking at buying U.K. startup for $1...   \n",
              "10  billion  billion  Apple is looking at buying U.K. startup for $1...   \n",
              "11        .        .  Apple is looking at buying U.K. startup for $1...   \n",
              "\n",
              "      POS  Tag       Dep  Shape  Is alpha  Is stop  Is punctuation     Head  \\\n",
              "0   PROPN  NNP     nsubj  Xxxxx      True    False           False  looking   \n",
              "1     AUX  VBZ       aux     xx      True     True           False  looking   \n",
              "2    VERB  VBG      ROOT   xxxx      True    False           False  looking   \n",
              "3     ADP   IN      prep     xx      True     True           False  looking   \n",
              "4    VERB  VBG     pcomp   xxxx      True    False           False       at   \n",
              "5   PROPN  NNP     nsubj   X.X.     False    False           False  startup   \n",
              "6    VERB  VBD     ccomp   xxxx      True    False           False   buying   \n",
              "7     ADP   IN      prep    xxx      True     True           False  startup   \n",
              "8     SYM    $  quantmod      $     False    False           False  billion   \n",
              "9     NUM   CD  compound      d     False    False           False  billion   \n",
              "10    NUM   CD      pobj   xxxx      True    False           False      for   \n",
              "11  PUNCT    .     punct      .     False    False            True  looking   \n",
              "\n",
              "              Children  \n",
              "0                   []  \n",
              "1                   []  \n",
              "2   [Apple, is, at, .]  \n",
              "3             [buying]  \n",
              "4            [startup]  \n",
              "5                   []  \n",
              "6          [U.K., for]  \n",
              "7            [billion]  \n",
              "8                   []  \n",
              "9                   []  \n",
              "10              [$, 1]  \n",
              "11                  []  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "This section demonstrates how to use spaCy to perform advanced linguistic analysis on a sentence.\n",
        "'''\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "token_info = []\n",
        "for token in doc:   # Iterate over each token in the spaCy Doc object\n",
        "    info = {        # Create a dictionary to store token information\n",
        "        \"Token\": token.text,    # The original token text\n",
        "        \"Lemma\": token.lemma_,  # The lemmatized (base) form of the token\n",
        "        \"Sentence\": token.sent.text,  # The full sentence containing the token\n",
        "        \"POS\": token.pos_,  # The coarse-grained part-of-speech tag\n",
        "        \"Tag\": token.tag_,  # The fine-grained part-of-speech tag\n",
        "        \"Dep\": token.dep_,  # The syntactic dependency label\n",
        "        \"Shape\": token.shape_,  # The shape of the token (e.g., Xxxx, xxxx, etc.)\n",
        "        \"Is alpha\": token.is_alpha,  # Boolean: is the token alphabetic?\n",
        "        \"Is stop\": token.is_stop,  # Boolean: is the token a stop word?\n",
        "        \"Is punctuation\": token.is_punct,  # Boolean: is the token punctuation?\n",
        "        \"Head\": token.head.text,  # The syntactic head token for this token\n",
        "        \"Children\": [child.text for child in token.children]  # List of child tokens in the dependency parse\n",
        "    }\n",
        "    token_info.append(info)  # Add the token's info dictionary to the list\n",
        "df = pd.DataFrame(token_info)\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text-Introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
