{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold\">Welcome to Natural language processing (NLP) in Python</span><br/>\n",
        "\n",
        "Presented by: Reza Saadatyar (2024-2025)<br/>\n",
        "E-mail: Reza.Saadatyar@outlook.com<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; font-weight: bold\">Outline:</span><br/>\n",
        "1️⃣ `Introduction NLP`<br/>\n",
        "▪ Overview of Natural Language Processing<br/>\n",
        "▪ Key areas: Text Analysis, Language Generation, Speech Processing, Semantic Understanding<br/>\n",
        "▪ NLP Challenges<br/>\n",
        "▪ Introduction to NLTK and spaCy libraries and their capabilities<br/>\n",
        "▪ NLTK & spaCy Setup<br/>\n",
        "2️⃣ `Understanding Text Dataset Structure in NLP`<br/>\n",
        "▪ Corpora<br/>\n",
        "▪ Corpus<br/>\n",
        "▪ Document<br/>\n",
        "▪ Token<br/>\n",
        "3️⃣ `Semantic, Syntactic, and Sentiment Analysis in NLP`<br/>\n",
        "▪ Semantic Analysis<br/>\n",
        "▪ Syntactic Analysis<br/>\n",
        "▪ Sentiment Analysis<br/>\n",
        "4️⃣ `Regular Expressions in NLP`<br/>\n",
        "5️⃣ `Stopwords Removal`<br/>\n",
        "6️⃣ `Chunking and Parsing in NLP`<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; font-weight: bold; color:rgb(255, 251, 18)\">1️⃣ Introduction NLP</span><br/>\n",
        "NLP is a branch of artificial intelligence that allows computers to understand, interpret, and generate human language by combining linguistics, computer science, and machine learning.<br/> \n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold\">Key Areas of NLP:</span><br/>\n",
        "`Text Analysis:`<br/>\n",
        "▪ Tokenization: Breaking text into words or sentences.<br/>\n",
        "▪ Part-of-Speech (POS) Tagging: Identifying grammatical components (e.g., nouns, verbs).<br/>\n",
        "▪ Named Entity Recognition (NER): Extracting entities like names, dates, or organizations.<br/>\n",
        "▪ Sentiment Analysis: Determining the emotional tone (positive, negative, neutral).<br/>\n",
        "\n",
        "`Language Generation:`<br/>\n",
        "▪ Text Summarization: Condensing long texts into shorter summaries.<br/>\n",
        "▪ Machine Translation: Converting text between languages (e.g., Google Translate).<br/>\n",
        "▪ Text Generation: Creating human-like text (e.g., chatbots, story generators).<br/>\n",
        "\n",
        "`Speech Processing:`<br/>\n",
        "▪ Speech Recognition: Converting spoken words to text (e.g., Siri, Alexa).<br/>\n",
        "▪ Text-to-Speech (TTS): Generating spoken language from text.<br/>\n",
        "▪ Voice Assistants: Combining speech recognition and NLP for interactive systems.<br/>\n",
        "\n",
        "`Semantic Understanding:`<br/>\n",
        "▪ Word Embeddings: Representing words as vectors (e.g., Word2Vec, BERT).<br/>\n",
        "▪ Question Answering: Providing precise answers to user queries.<br/>\n",
        "▪ Dialogue Systems: Enabling conversational agents to maintain context.<br/>\n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold\">NLP Challenges:</span><br/>\n",
        "▪ `Ambiguity & Context Sensitivity:`<br/>\n",
        "Human language is often ambiguous, meaning that the same word or sentence can have multiple meanings depending on the context. For example, the word \"bank\" could refer to a financial institution or the side of a river. NLP systems must use context to resolve these ambiguities and understand the intended meaning.<br/>\n",
        "▪ `Cultural & Linguistic Nuances:`<br/>\n",
        "Language varies greatly across different cultures, regions, and social groups. Idioms, slang, humor, and cultural references can be difficult for NLP systems to interpret correctly. Additionally, languages have unique grammatical structures and vocabulary that require specialized handling.<br/>\n",
        "▪ `Handling Massive Datasets:`<br/>\n",
        "Modern NLP applications often need to process and analyze huge volumes of text data, such as social media posts, news articles, or customer reviews. Efficient algorithms and scalable infrastructure are necessary to manage, store, and analyze this data in a reasonable amount of time.<br/>\n",
        "▪ `Continuous Innovation for Greater Accuracy & Relevance:`<br/>\n",
        "The field of NLP is rapidly evolving, with new models and techniques being developed to improve accuracy and relevance. Staying up-to-date with the latest research, adapting to new data sources, and refining models are ongoing challenges to ensure NLP systems remain effective and reliable.<br/>\n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold;\">NLTK and spaCy libraries</span><br/>\n",
        "▪ [NLTK](https://www.nltk.org/) is a comprehensive open-source Python library designed for *educational* and *research* purposes in natural language processing. It provides robust tools for tasks like tokenization, stemming, lemmatization, parsing, and more, backed by extensive corpora and lexical resources.<br/> \n",
        "▪ [spaCy](https://spacy.io/) is an *industrial-strength* NLP library in Python tailored for real-world applications, emphasizing speed and accuracy in tasks such as tokenization, named entity recognition, and dependency parsing. Its modern API and efficient design make it ideal for processing large-scale text data.<br/> \n",
        "\n",
        "• `Classification:` Assigning predefined categories or labels to text, such as spam detection, sentiment analysis, or topic categorization.<br/>\n",
        "• `Tokenization:` Breaking text into smaller units (words, sentences, or phrases)<br/>\n",
        "• `Stemming:` Reducing words to their root form by removing suffixes/prefixes<br/>\n",
        "• `Tagging:` Assigning grammatical categories (POS tags) to words<br/>\n",
        "• `Parsing:` Analyzing sentence structure and grammatical relationships<br/>\n",
        "• `Semantic Reasoning:` Understanding meaning and relationships between words/concepts<br/>\n",
        "• `Wrappers:` Interface layers that connect to powerful NLP libraries like spaCy or Stanford NLP<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold\">NLTK & spaCy Setup:</span><br/>\n",
        "• `NLTK's Punkt` is an unsupervised sentence tokenizer that segments text into sentences by learning punctuation and abbreviation patterns from data. In contrast, spaCy provides its own statistical sentence segmentation, which is fast and effective for a variety of text types.<br/>\n",
        "• The `en_core_web_sm model` in spaCy is a small, efficient English model supporting tokenization, part-of-speech tagging, dependency parsing, and named\n",
        "entity recognition. For greater accuracy and more features, you can use larger models like en_core_web_md, en_core_web_lg, or the transformer-based \n",
        "en_core_web_trf.<br/>\n",
        "• `WordNet` is a comprehensive lexical database within NLTK that organizes English words into synonym sets (synsets) to facilitate semantic analysis; alternatives like BabelNet or ConceptNet extend these capabilities to multilingual and broader semantic relationships.<br/>\n",
        "• The `Gutenberg` corpus in NLTK is a curated collection of classic literary texts sourced from Project Gutenberg, offering diverse works for exploring historical language patterns and stylistic nuances; alternatives like the Brown Corpus or Reuters Corpus provide additional perspectives for varied text analysis.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install spacy\n",
        "! pip install nltk\n",
        "! pip install regex==2023.10.3\n",
        "! pip install spacy-wordnet\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<span style=\"font-size: 16px; color: rgb(11, 7, 241); font-weight: bold\">2️⃣ Understanding Text Dataset Structure in NLP</span><br/>\n",
        "▪ `Corpora:` A *corpus* (plural: *corpora*) is a large and structured set of texts, often used for linguistic analysis or to train NLP models. A corpus can contain thousands or millions of documents (multiple datasets).<br/>\n",
        "▪ `Corpus:` Sometimes, the term \"corpus\" is used to refer to a single collection of documents, while \"corpora\" refers to multiple such collections (one dataset).<br/>\n",
        "▪ `Document:` A document is an individual piece of text within a corpus, such as an article, a book, a tweet, or an email (one text).<br/>\n",
        "▪ `Token:`A token is the smallest unit of text, typically a word, punctuation mark, or symbol, obtained after tokenization. Tokenization is the process of splitting text into these basic units (word, punctuation, etc.).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents in the webtext corpus:\n",
            "• firefox.txt\n",
            "• grail.txt\n",
            "• overheard.txt\n",
            "• pirates.txt\n",
            "• singles.txt\n",
            "• wine.txt\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import webtext # Provides access to the Webtext corpus, useful for training and testing tokenizers\n",
        "\n",
        "corpus = webtext # The entire webtext corpus\n",
        "documents = corpus.fileids()   # List all documents (fileids) in the corpus\n",
        "print(\"Documents in the webtext corpus:\")\n",
        "for doc in documents:\n",
        "    print(f\"• {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Document: firefox.txt\n",
            "\n",
            "First 300 characters of the document:\n",
            "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
            "When in full screen mode\n",
            "Pressing Ctrl-N should open a new browser when only download dialog is left open\n",
            "add icons to context menu\n",
            "So called \"tab bar\" should be made a proper toolbar or given \n"
          ]
        }
      ],
      "source": [
        "document_id = documents[0]  # Select a document (e.g., 'grail.txt')\n",
        "print(f\"Selected Document: {document_id}\")\n",
        "\n",
        "raw_text = corpus.raw(document_id)  # Get the raw text of the document\n",
        "print(f\"\\nFirst 300 characters of the document:\\n{raw_text[:300]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences in the document: 1142\n",
            "\n",
            "First sentence:\n",
            "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
            "When in full screen mode\n",
            "Pressing Ctrl-N should open a new browser when only download dialog is left open\n",
            "add icons to context menu\n",
            "So called \"tab bar\" should be made a proper toolbar or given the ability collapse / expand.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize  # Function for splitting text into sentences using a pre-trained model\n",
        "\n",
        "sentences = sent_tokenize(raw_text)      # Tokenize the document into sentences\n",
        "print(f\"Number of sentences in the document: {len(sentences)}\")\n",
        "print(f\"\\nFirst sentence:\\n{sentences[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the document: 96120\n",
            "First 10 tokens:\n",
            "['Cookie', 'Manager', ':', '``', 'Do', \"n't\", 'allow', 'sites', 'that', 'set']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "tokens = word_tokenize(raw_text)    # Tokenize the document into words\n",
        "print(f\"Number of tokens in the document: {len(tokens)}\")\n",
        "print(f\"First 10 tokens:\\n{tokens[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px;  color:rgb(38, 255, 18); font-weight: bold\">3️⃣ Semantic, Syntactic, and Sentiment Analysis in NLP</span><br/>\n",
        "▪ `Semantic Analysis:` Understanding the meaning of words, phrases, and sentences in context. Semantic analysis helps computers grasp what a text is actually talking about, such as identifying relationships between entities or resolving ambiguity in meaning.<br/>\n",
        "▪ `Syntactic Analysis:` Examining the grammatical structure of sentences. Syntactic analysis (or parsing) helps determine how words are related to each other in a sentence, such as identifying subjects, verbs, and objects, and ensuring the sentence follows the rules of grammar.<br/>\n",
        "▪ `Sentiment Analysis:` Determining the emotional tone behind a body of text. Sentiment analysis is used to identify whether the sentiment expressed is positive, negative, or neutral, which is especially useful in applications like social media monitoring or customer feedback analysis.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Analysis: Synsets for 'bank':\n",
            "• bank.n.01: sloping land (especially the slope beside a body of water)\n",
            "• depository_financial_institution.n.01: a financial institution that accepts deposits and channels the money into lending activities\n",
            "• bank.n.03: a long ridge or pile\n",
            "• bank.n.04: an arrangement of similar objects in a row or in tiers\n",
            "• bank.n.05: a supply or stock held in reserve for future use (especially in emergencies)\n",
            "• bank.n.06: the funds held by a gambling house or the dealer in some gambling games\n",
            "• bank.n.07: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
            "• savings_bank.n.02: a container (usually with a slot in the top) for keeping money at home\n",
            "• bank.n.09: a building in which the business of banking transacted\n",
            "• bank.n.10: a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
            "• bank.v.01: tip laterally\n",
            "• bank.v.02: enclose with a bank\n",
            "• bank.v.03: do business with a bank or keep an account at a bank\n",
            "• bank.v.04: act as the banker in a game or in gambling\n",
            "• bank.v.05: be in the banking business\n",
            "• deposit.v.02: put into a bank account\n",
            "• bank.v.07: cover with ashes so to control the rate of burning\n",
            "• trust.v.01: have confidence or faith in\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet  # Provides access to the WordNet lexical database for lemmatization and semantic analysis\n",
        "\n",
        "word = 'bank'  # Define the word to analyze semantically\n",
        "synsets = wordnet.synsets(word)  # Retrieve all synsets (senses) for the word from WordNet\n",
        "\n",
        "print(f\"Semantic Analysis: Synsets for '{word}':\") \n",
        "for syn in synsets:  # Iterate over each synset for the word\n",
        "    print(f\"• {syn.name()}: {syn.definition()}\")  # Print the synset name and its definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syntactic Analysis: POS Tags for the sentence:\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize   # Function for splitting text into words (and punctuation)\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(sentence)  # Tokenize the sentence into words\n",
        "\n",
        "pos_tags = nltk.pos_tag(tokens)  # Perform part-of-speech tagging on the tokens\n",
        "print(f\"Syntactic Analysis: POS Tags for the sentence:\\n{pos_tags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Analysis: Sentiment scores for the text:\n",
            "{'neg': 0.0, 'neu': 0.221, 'pos': 0.779, 'compound': 0.9336}\n",
            "\n",
            "WordNet Synsets for 'great':\n",
            "[Synset('great.n.01'), Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.13')]\n",
            "\n",
            "Gutenberg Files:\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg  # import the Gutenberg corpus from NLTK\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer  # import the SentimentIntensityAnalyzer for sentiment analysis\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()  # create an instance of SentimentIntensityAnalyzer\n",
        "text = \"I love natural language processing! It's amazing and fun.\"  # sample text for sentiment analysis\n",
        "sentiment_scores = sia.polarity_scores(text)  # get sentiment polarity scores for the text\n",
        "print(f\"Sentiment Analysis: Sentiment scores for the text:\\n{sentiment_scores}\")  # print the sentiment scores\n",
        "\n",
        "\n",
        "# Get synsets (sets of cognitive synonyms) for the word 'great' from WordNet\n",
        "synsets = wordnet.synsets('great')  # retrieve synsets for the word 'great' from WordNet\n",
        "print(f\"\\nWordNet Synsets for 'great':\\n{synsets}\")  # print the synsets for 'great'\n",
        "\n",
        "# list available files in the Gutenberg corpus\n",
        "print(f\"\\nGutenberg Files:\\n{gutenberg.fileids()}\")  # print the list of available files in the Gutenberg corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: rgb(237, 4, 245); font-weight: bold\">4️⃣ Regular Expressions in NLP</span><br/>\n",
        "Regular expressions (regex) are powerful tools used to identify, extract, and manipulate specific patterns within text data.<br/>\n",
        "▪ `Pattern Matching:` Find and match specific sequences of characters (e.g., email addresses, dates, phone numbers) in text.<br/>\n",
        "▪ `Substring Extraction:` Pull out relevant parts of text that match a defined pattern.<br/>\n",
        "▪ `Pattern Replacement/Removal:` Substitute or eliminate text patterns, such as removing unwanted symbols or correcting formats.<br/>\n",
        "▪ `Noise Filtering:` Clean text by filtering out irrelevant or noisy data, making it more suitable for analysis.<br/>\n",
        "\n",
        "<span style=\"font-size: 15.5px; ; font-weight: bold\">Character Ranges and Quantifiers:</span><br/> \n",
        "▪ `[A-Za-z]` matches any uppercase or lowercase letter.<br/> \n",
        "▪ `{2}` means exactly 2 occurrences of the preceding pattern.<br/> \n",
        "▪ `\\d{3}` matches exactly 3 digits (where `\\d` is any digit from 0 to 9).<br/> \n",
        "▪  Square brackets `[]` define a set or range of characters to match.<br/> \n",
        "\n",
        "<span style=\"font-size: 15.5px; ; font-weight: bold\">Example:</span><br/> \n",
        "^[\\w\\.-]+@([\\w-]+\\.)+[\\w-]{2,4}\n",
        "\n",
        "`^[\\w\\.-]+`: Matches the start of the string and then one or more word characters (`\\w`), dots (`.`), or hyphens (`-`). This is the username part of the email.<br/>\n",
        "`@` : Matches the literal '@' symbol.<br/> \n",
        "`([\\w-]+\\.)+` : Matches one or more groups of word characters or hyphens followed by a dot. This covers subdomains and the main domain.<br/> \n",
        "`[\\w-]{2,4}$` : Matches the top-level domain (TLD) at the end, which must be 2 to 4 word characters or hyphens.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Regex Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '123', '252', 'times']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer  # Import RegexpTokenizer from NLTK\n",
        "\n",
        "example_text = \"The quick brown fox jumps over the lazy dog, 123-252 times!\"  # Example text to tokenize\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # Create a tokenizer that matches words (alphanumeric sequences)\n",
        "tokens = tokenizer.tokenize(example_text)  # Tokenize the example text using the regex tokenizer\n",
        "print(\"NLTK Regex Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected Emails: ['admin.support_34@example.com', 'sales-dep@company.org']\n"
          ]
        }
      ],
      "source": [
        "import re  # Import the regular expressions module\n",
        "text_emails = \"Contact us at admin.support_34@example.com or sales-dep@company.org for inquiries.\"  # Example text containing email addresses\n",
        "\n",
        "# Define a regular expression pattern to match email addresses:\n",
        "# [a-zA-Z0-9._%+-]+   : Matches one or more allowed characters in the username part (letters, digits, dot, underscore, percent, plus, hyphen)\n",
        "# @                   : Matches the '@' symbol\n",
        "# [a-zA-Z0-9.-]+      : Matches one or more allowed characters in the domain name (letters, digits, dot, hyphen)\n",
        "# \\.                  : Matches a literal dot before the domain extension\n",
        "# [a-zA-Z]{2,}        : Matches the domain extension (at least two letters)\n",
        "email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "\n",
        "emails = re.findall(email_pattern, text_emails)  # Find all email addresses in the text using the regex pattern\n",
        "print(\"Detected Emails:\", emails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capitalized words found in the sentence:\n",
            "• Alice\n",
            "• Bob\n",
            "• New\n",
            "• York\n",
            "• City\n",
            "• Friday\n"
          ]
        }
      ],
      "source": [
        "import spacy  # Import the spaCy library for advanced NLP tasks\n",
        "from spacy.matcher import Matcher  # Import the Matcher class for pattern matching in spaCy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model\n",
        "matcher = Matcher(nlp.vocab)        # Create a Matcher object\n",
        "\n",
        "# Define a pattern to match words that start with a capital letter\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}}]\n",
        "matcher.add(\"CAPITALIZED_WORD\", [pattern])\n",
        "\n",
        "text = \"Alice and Bob went to New York City last Friday.\" # Example sentence\n",
        "doc = nlp(text)  # Process the text\n",
        "matches = matcher(doc)  # Apply the matcher to the doc\n",
        "\n",
        "# Print the matched tokens\n",
        "print(\"Capitalized words found in the sentence:\")\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(\"•\", span.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font*size: 16px; color: rgb(6, 168, 243); font-weight: bold\">5️⃣ Stopwords Removal</span><br/>\n",
        "Stopwords are common words (such as \"the\", \"is\", \"in\", \"and\") that are often removed from text before processing, as they typically do not carry significant meaning. Filtering out stopwords helps reduce noise and improves the efficiency of text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Words: ['example', 'sentence', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"This is an example sentence showing off the stop words filtration.\"\n",
        "words = nltk.word_tokenize(text)  # tokenize the text into words using nltk\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]  # filter out stopwords\n",
        "print(\"Filtered Words:\", filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "This is an example sentence showing off the stop words filtration.\n",
            "\n",
            "After Stopwords Removal:\n",
            "example sentence showing stop words filtration .\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model\n",
        "doc = nlp(text)  # Process the text with spaCy\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]  # Create a list of tokens that are not stopwords\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nAfter Stopwords Removal:\")  # Print the label for filtered text\n",
        "print(\" \".join(filtered_tokens))  # Print the filtered text without stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: rgb(12, 238, 125); font-weight: bold\">Part-of-Speech (POS) Tagging in NLP</span><br/>\n",
        "POS tagging is the process of labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc. This helps computers understand the grammatical structure and meaning of text.<br/>\n",
        "\n",
        "<span style=\"font-size: 15.5px; font-weight: bold\">Common POS Tags and Examples:</span><br/>\n",
        "▪ `Noun (N):` Names of people, places, things, or ideas.<br/>\n",
        "▪ `Verb (V):` Words that express actions or states.<br/>\n",
        "▪ `Adjective (ADJ):` Words that describe nouns.<br/>\n",
        "▪ `Adverb (ADV):` Words that modify verbs, adjectives, or other adverbs.<br/>\n",
        "▪ `Preposition (P):` Words that show relationships between nouns or pronouns and other words (at, on, in, from, with, near, between, about, under).<br/>\n",
        "▪ `Conjunction (CON):` Words that connect clauses, sentences, or words (and, or, but, because, so, yet, unless, since, if).<br/>\n",
        "▪ `Pronoun (PRO):` Words that replace nouns (you, I, we, they, he, she, it, me, us, them, him, her, this).<br/>\n",
        "▪ `Interjection (INT):` Words or phrases that express emotion or exclamation (Ouch! Wow! Great! Help! Oh! Hey! Hi!).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Tags using NLTK:/n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "\n",
            "POS Tags using spaCy:[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence) # Tokenize the sentence\n",
        "pos_tags = nltk.pos_tag(tokens) # POS tagging using NLTK\n",
        "print(f\"POS Tags using NLTK:/n{pos_tags}\")\n",
        "\n",
        "doc = nlp(sentence) # POS tagging using spaCy\n",
        "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(f\"\\nPOS Tags using spaCy:{spacy_pos_tags}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: rgb(171, 12, 245); font-weight: bold\">6️⃣ Chunking and Parsing in NLP</span><br/>\n",
        "Chunking and parsing are essential techniques in NLP for understanding the structure and meaning of sentences.<br/>\n",
        "▪ `Group words into meaningful chunks:` Chunking segments sentences into groups of words (such as noun or verb phrases) that function together as a unit.<br/>\n",
        "▪ `Identify phrase boundaries:` These methods help determine where phrases begin and end, making it easier to extract relevant information.<br/>\n",
        "▪ `Analyze grammatical structure:` Parsing examines the grammatical relationships between words and phrases, revealing how sentences are constructed.<br/>\n",
        "▪ `Reveal sentence hierarchy:` Parsing uncovers the hierarchical structure of a sentence, showing how smaller chunks combine to form larger grammatical units.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunked Sentence Structure:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n",
            "                                S                                          \n",
            "     ___________________________|_______________________________            \n",
            "    |        |     |            NP               NP             NP         \n",
            "    |        |     |     _______|________        |       _______|______     \n",
            "jumps/VBZ over/IN ./. The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize and POS tag the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define a chunk grammar for noun phrases (NP)\n",
        "# The pattern \"NP: {<DT>?<JJ>*<NN>}\" can be broken down as follows:\n",
        "# - NP: This is the label for the chunk (Noun Phrase).\n",
        "# - { ... }: The curly braces enclose the pattern to match.\n",
        "# - <DT>? : An optional determiner (e.g., \"the\", \"a\"). The question mark means it may appear 0 or 1 time.\n",
        "# - <JJ>* : Zero or more adjectives (e.g., \"quick\", \"brown\"). The asterisk means any number of adjectives can appear.\n",
        "# - <NN>  : A singular noun (e.g., \"fox\", \"dog\"). This is required at the end of the pattern.\n",
        "# This grammar will match sequences like \"the quick brown fox\" or \"lazy dog\" as noun phrases.\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)   # Create a RegexpParser object and parse the tagged tokens\n",
        "\n",
        "# Parse the POS-tagged sentence to get chunks\n",
        "tree = cp.parse(pos_tags)\n",
        "\n",
        "# Print the chunked structure\n",
        "print(\"Chunked Sentence Structure:\")\n",
        "print(tree)\n",
        "\n",
        "tree.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Noun Chunks List:\n",
            "['The quick brown fox', 'the lazy dog']\n",
            "\n",
            "NLTK Noun Chunks List:\n",
            "['The quick brown', 'fox', 'the lazy dog']\n"
          ]
        }
      ],
      "source": [
        "sen = nlp(sentence)  # Process the sentence using spaCy's NLP pipeline\n",
        "spacy_noun_chunks = [chunk.text for chunk in sen.noun_chunks]  # Extract noun chunks from the processed sentence using spaCy\n",
        "print(f\"spaCy Noun Chunks List:\\n{spacy_noun_chunks}\")\n",
        "\n",
        "# Create a list of noun chunks using NLTK (from the previous chunked tree)\n",
        "nltk_noun_chunks = []  # Initialize an empty list to store NLTK noun chunks\n",
        "for subtree in tree.subtrees():  # Iterate over all subtrees in the chunked tree\n",
        "    if subtree.label() == 'NP':  # Check if the subtree is labeled as a noun phrase (NP)\n",
        "        chunk = \" \".join(word for word, pos in subtree.leaves())  # Join the words in the noun phrase chunk\n",
        "        nltk_noun_chunks.append(chunk)  # Add the noun phrase chunk to the list\n",
        "print(f\"\\nNLTK Noun Chunks List:\\n{nltk_noun_chunks}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: #eb5e28; font-weight: bold\">Hypernyms and Hyponyms in NLP</span><br/>\n",
        "▪ `Hypernyms` are words that denote a broad category or general class (e.g., \"animal\" is a hypernym of \"dog\").<br/>\n",
        "▪ `Hyponyms` are words that represent a more specific instance within a category (e.g., \"poodle\" is a hyponym of \"dog\").<br/>\n",
        "Recognizing hypernyms and hyponyms helps build a *semantic hierarchy*, allowing NLP systems to understand relationships between general and specific terms.<br/>\n",
        "This enhances *lexical organization* and improves tasks like information retrieval, question answering, and semantic search by enabling systems to group, relate, and infer meaning from word relationships.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synset for 'dog': dog.n.01 - a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
            "\n",
            "Hypernyms:\n",
            "• canine.n.02 - any of various fissiped mammals with nonretractile claws and typically long muzzles\n",
            "• domestic_animal.n.01 - any of various animals that have been tamed and made fit for a human environment\n",
            "\n",
            "Hyponyms:\n",
            "• basenji.n.01 - small smooth-haired breed of African origin having a tightly curled tail and the inability to bark\n",
            "• corgi.n.01 - either of two Welsh breeds of long-bodied short-legged dogs with erect ears and a fox-like head\n",
            "• cur.n.01 - an inferior dog or one of mixed breed\n",
            "• dalmatian.n.02 - a large breed having a smooth white coat with black or brown spots; originated in Dalmatia\n",
            "• great_pyrenees.n.01 - bred of large heavy-coated white dogs resembling the Newfoundland\n"
          ]
        }
      ],
      "source": [
        "word = \"dog\"  # Define the word to look up in WordNet\n",
        "synsets = wordnet.synsets(word, pos=wordnet.NOUN)  # Get all noun synsets for the word\n",
        "\n",
        "if synsets:  # Check if any synsets were found\n",
        "    syn = synsets[0]  # Take the first synset as an example\n",
        "    print(f\"Synset for '{word}': {syn.name()} - {syn.definition()}\")  # Print the synset name and definition\n",
        "    \n",
        "    # Get hypernyms (more general terms)\n",
        "    hypernyms = syn.hypernyms()  # Retrieve hypernyms for the synset\n",
        "    print(\"\\nHypernyms:\")\n",
        "    for h in hypernyms:  # Iterate over each hypernym\n",
        "        print(f\"• {h.name()} - {h.definition()}\")  # Print the hypernym name and definition\n",
        "    \n",
        "    # Get hyponyms (more specific terms)\n",
        "    hyponyms = syn.hyponyms()  # Retrieve hyponyms for the synset\n",
        "    print(\"\\nHyponyms:\")\n",
        "    for h in hyponyms[:5]:  # Show only first 5 hyponyms for brevity\n",
        "        print(f\"• {h.name()} - {h.definition()}\")  # Print the hyponym name and definition\n",
        "else:  # If no synsets were found\n",
        "    print(f\"No synsets found for '{word}'.\")  # Print a message indicating no synsets found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px; color: #cbaf89; font-weight: bold\">Named Entity Recognition (NER)</span><br/>\n",
        "NER is a fundamental task in NLP that involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, dates, and more.<br/>\n",
        "NER helps extract structured information from unstructured text, enabling applications like information extraction, question answering, and knowledge graph construction.<br/>\n",
        "Modern NLP libraries like spaCy and NLTK provide built-in tools for performing NER efficiently.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy NER Results:\n",
            "• Barack Obama (PERSON)\n",
            "• Hawaii (GPE)\n",
            "• 44th (ORDINAL)\n",
            "• the United States (GPE)\n"
          ]
        }
      ],
      "source": [
        "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy NER Results:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"• {ent.text} ({ent.label_})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16px;font-weight:bold\"> Stemming & Lemmatization</span><br/>\n",
        "Stemming and Lemmatization are two fundamental techniques in NLP used to reduce words to their root or base forms.<br/>\n",
        "\n",
        "**Stemming:**<br/>\n",
        "▪ Stemming is the process of removing suffixes (and sometimes prefixes) from words to obtain their stem or root form.<br/>\n",
        "▪ The resulting stem may not always be a valid word in the language, but it helps group together words with similar meanings (e.g., \"playing\", \"played\", \"plays\" → \"play\").<br/>\n",
        "▪ Stemming algorithms are typically rule-based and fast, but can be less accurate.<br/>\n",
        "\n",
        "**Lemmatization:**<br/>\n",
        "▪ Lemmatization reduces words to their base or dictionary form, known as the lemma.<br/>\n",
        "▪ Unlike stemming, lemmatization considers the context and part of speech of a word, ensuring that the root form is a valid word (e.g., \"better\" → \"good\", \"running\" → \"run\").<br/>\n",
        "▪ Lemmatization is generally more accurate but may require more computational resources and linguistic knowledge.<br/>\n",
        "\n",
        "**Workflow for Stemming and Lemmatization:**<br/>\n",
        "▪ `Lowercasing:` Convert all text to lowercase for consistency.<br/>\n",
        "▪ `Tokenization:` Split text into individual words (tokens).<br/>\n",
        "▪ `Stemming/Lemmatization:` Apply stemming or lemmatization to each token to obtain root forms.<br/>\n",
        "▪ `Reconstruction (Optional):` Reconstruct the processed tokens back into text for further analysis.<br/>\n",
        "\n",
        "These techniques are commonly used in text preprocessing to normalize words, improve search results, and enhance the performance of NLP models.<br/>\n",
        "\n",
        "**Difference between Stemming and Lemmatization (with Lancaster Stemmer):**<br/>\n",
        "\n",
        "The main difference between stemming and lemmatization is that stemming crudely removes word suffixes to arrive at a root form, which may not be a valid word, while lemmatization reduces words to their dictionary form (lemma), considering context and part of speech.<br/>\n",
        "\n",
        "**Stemming (Lancaster):**<br/>\n",
        "▪ The Lancaster stemmer is more aggressive than the Porter stemmer, often producing shorter stems.<br/>\n",
        "▪ Example: \"playing\", \"played\", \"plays\" → \"play\" (Porter), but Lancaster may reduce further.<br/>\n",
        "\n",
        "**Lemmatization:**<br/>\n",
        "▪ Lemmatization always returns a valid word (lemma) and is context-aware.<br/>\n",
        "▪ Example: \"better\" → \"good\" (with POS), \"running\" → \"run\".<br/>\n",
        "\n",
        "**Comparison Table:**<br/>\n",
        "| Word      | Lancaster Stem | Porter Stem | Lemma      |\n",
        "|-----------|:--------------|:------------|:-----------|\n",
        "| playing   | play           | play        | playing    |\n",
        "| played    | play           | play        | played     |\n",
        "| plays     | play           | play        | play       |\n",
        "| better    | bet            | better      | better     |\n",
        "| running   | run            | run         | running    |\n",
        "\n",
        "The Lancaster stemmer can be too aggressive for some applications, while lemmatization is more accurate but slower.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>tokens</th>\n",
              "      <th>stems</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cats are running</td>\n",
              "      <td>[cats, are, running]</td>\n",
              "      <td>[cat, are, run]</td>\n",
              "      <td>[cat, are, running]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dogs played outside</td>\n",
              "      <td>[dogs, played, outside]</td>\n",
              "      <td>[dog, play, outsid]</td>\n",
              "      <td>[dog, played, outside]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              original                   tokens                stems  \\\n",
              "0     Cats are running     [cats, are, running]      [cat, are, run]   \n",
              "1  Dogs played outside  [dogs, played, outside]  [dog, play, outsid]   \n",
              "\n",
              "                   lemmas  \n",
              "0     [cat, are, running]  \n",
              "1  [dog, played, outside]  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "documents = [\n",
        "    \"Cats are running\",\n",
        "    \"Dogs played outside\",\n",
        "]\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()  # Create a stemmer object using the Porter algorithm\n",
        "lemmatizer = WordNetLemmatizer() # Create a lemmatizer object using WordNet\n",
        "\n",
        "# Tokenize, Stem, and Lemmatize each document\n",
        "results = []  # Initialize an empty list to store results for each document\n",
        "for doc in documents:  # Iterate over each document in the documents list\n",
        "    tokens = word_tokenize(doc.lower())  # Tokenize the document after converting it to lowercase\n",
        "    stems = [stemmer.stem(token) for token in tokens]  # Apply stemming to each token\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]  # Apply lemmatization to each token\n",
        "    results.append({  # Append a dictionary with original text, tokens, stems, and lemmas to the results list\n",
        "        \"original\": doc,  # Store the original document text\n",
        "        \"tokens\": tokens,  # Store the list of tokens\n",
        "        \"stems\": stems,  # Store the list of stemmed tokens\n",
        "        \"lemmas\": lemmas  # Store the list of lemmatized tokens\n",
        "    })\n",
        "\n",
        "# Display results in a DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stemming vs Lemmatization Comparison:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>playing</td>\n",
              "      <td>play</td>\n",
              "      <td>playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>played</td>\n",
              "      <td>play</td>\n",
              "      <td>played</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plays</td>\n",
              "      <td>play</td>\n",
              "      <td>play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>running</td>\n",
              "      <td>run</td>\n",
              "      <td>running</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word    stem    lemma\n",
              "0  playing    play  playing\n",
              "1   played    play   played\n",
              "2    plays    play     play\n",
              "3   better  better   better\n",
              "4  running     run  running"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compare stemming and lemmatization for a few words\n",
        "sample_words = [\"playing\", \"played\", \"plays\", \"better\", \"running\", \"feet\"]\n",
        "comparison = []\n",
        "for word in sample_words:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    comparison.append({\"word\": word, \"stem\": stem, \"lemma\": lemma})\n",
        "\n",
        "print(\"\\nStemming vs Lemmatization Comparison:\")\n",
        "df = pd.DataFrame(comparison)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stemming vs Lemmatization Comparison:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>playing</td>\n",
              "      <td>play</td>\n",
              "      <td>playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>played</td>\n",
              "      <td>play</td>\n",
              "      <td>played</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plays</td>\n",
              "      <td>play</td>\n",
              "      <td>play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "      <td>better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>running</td>\n",
              "      <td>run</td>\n",
              "      <td>running</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word    stem    lemma\n",
              "0  playing    play  playing\n",
              "1   played    play   played\n",
              "2    plays    play     play\n",
              "3   better  better   better\n",
              "4  running     run  running"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: Compare stemming and lemmatization for a few words\n",
        "sample_words = [\"playing\", \"played\", \"plays\", \"better\", \"running\", \"feet\"]\n",
        "comparison = []\n",
        "for word in sample_words:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    comparison.append({\"word\": word, \"stem\": stem, \"lemma\": lemma})\n",
        "\n",
        "print(\"\\nStemming vs Lemmatization Comparison:\")\n",
        "df = pd.DataFrame(comparison)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "spaCy Lemmatization and POS Tagging:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>tokens</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cats are running</td>\n",
              "      <td>[Cats, are, running]</td>\n",
              "      <td>[cat, be, run]</td>\n",
              "      <td>[NOUN, AUX, VERB]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dogs played outside</td>\n",
              "      <td>[Dogs, played, outside]</td>\n",
              "      <td>[dog, play, outside]</td>\n",
              "      <td>[NOUN, VERB, ADV]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              original                   tokens                lemmas  \\\n",
              "0     Cats are running     [Cats, are, running]        [cat, be, run]   \n",
              "1  Dogs played outside  [Dogs, played, outside]  [dog, play, outside]   \n",
              "\n",
              "                 pos  \n",
              "0  [NOUN, AUX, VERB]  \n",
              "1  [NOUN, VERB, ADV]  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the small English model in spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentences\n",
        "spacy_docs = [\n",
        "    \"Cats are running\",\n",
        "    \"Dogs played outside\",\n",
        "]\n",
        "\n",
        "# Process each document with spaCy\n",
        "spacy_results = []\n",
        "for doc in spacy_docs:\n",
        "    spacy_doc = nlp(doc)\n",
        "    tokens = [token.text for token in spacy_doc]\n",
        "    lemmas = [token.lemma_ for token in spacy_doc]\n",
        "    pos = [token.pos_ for token in spacy_doc]\n",
        "    spacy_results.append({\n",
        "        \"original\": doc,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas,\n",
        "        \"pos\": pos\n",
        "    })\n",
        "\n",
        "# Display results in a DataFrame\n",
        "spacy_df = pd.DataFrame(spacy_results)\n",
        "print(\"\\nspaCy Lemmatization and POS Tagging:\")\n",
        "spacy_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:rgb(255, 0, 157); font-size: 16.5px; font-weight: bold\">Tokenization Concepts</span><br/>\n",
        "Tokenization is the process of breaking down text into smaller units called tokens. It's a fundamental step in NLP that helps computers understand and process human language by converting text into a format they can work with.<br/>\n",
        "\n",
        "<span style=\"font-size: 16.5px; font-weight: bold\">Types of tokenization:</span><br/>\n",
        "▪ `Sentence Tokenization:` Splits text into individual sentences, useful for document-level analysis<br/>\n",
        "▪ `Word Tokenization:` Breaks text into individual words, essential for word-level processing<br/>\n",
        "▪ `Regex Tokenization:` Uses regular expressions to extract specific patterns from text<br/>\n",
        "▪ `Treebank Tokenization:` Follows Penn Treebank conventions for standardized word tokenization<br/>\n",
        "▪ `WordPunct Tokenization:` Separates words and punctuation into distinct tokens<br/>\n",
        "▪ `Whitespace Tokenization:` Splits text based on spaces, the simplest form of tokenization<br/>\n",
        "▪ `Character Tokenization:` Breaks text into individual characters, useful for character-level analysis<br/>\n",
        "\n",
        "The [Webtext Corpus](https://paperswithcode.com/dataset/webtext) is a high-quality dataset that can be used to train custom tokenizers. It contains diverse text samples that help create robust tokenization models capable of handling various text patterns and formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence tokenization:\n",
            "['I am learning Natural Language Processing.', \"I'm learning Python programming.\", 'It is very user friendly.', \"I'm ready to start coding.\"]\n",
            "\n",
            "Word tokenization:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.', 'I', \"'m\", 'learning', 'Python', 'programming', '.', 'It', 'is', 'very', 'user', 'friendly', '.', 'I', \"'m\", 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "Regex tokenization (words only):\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', 'I', 'm', 'learning', 'Python', 'programming', 'It', 'is', 'very', 'user', 'friendly', 'I', 'm', 'ready', 'to', 'start', 'coding']\n",
            "\n",
            "TreebankWordTokenizer:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing.', 'I', \"'m\", 'learning', 'Python', 'programming.', 'It', 'is', 'very', 'user', 'friendly.', 'I', \"'m\", 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "WordPunctTokenizer:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.', 'I', \"'\", 'm', 'learning', 'Python', 'programming', '.', 'It', 'is', 'very', 'user', 'friendly', '.', 'I', \"'\", 'm', 'ready', 'to', 'start', 'coding', '.']\n",
            "\n",
            "Whitespace tokenization:\n",
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing.', \"I'm\", 'learning', 'Python', 'programming.', 'It', 'is', 'very', 'user', 'friendly.', \"I'm\", 'ready', 'to', 'start', 'coding.']\n",
            "\n",
            "Character tokenization:\n",
            "['I', ' ', 'a', 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '.', ' ', 'I', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'u', 's', 'e', 'r', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 'l', 'y', '.', ' ', 'I', \"'\", 'm', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 's', 't', 'a', 'r', 't', ' ', 'c', 'o', 'd', 'i', 'n', 'g', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer     # Class that splits text into word and punctuation tokens separately\n",
        "\n",
        "txt = \"I am learning Natural Language Processing. I'm learning Python programming. It is very user friendly. I'm ready to start coding.\"\n",
        "\n",
        "# Using sent_tokenize to split text into sentences\n",
        "# This is useful when you need to process text at the sentence level\n",
        "sent_tok = sent_tokenize(txt)\n",
        "print(f\"Sentence tokenization:\\n{sent_tok}\")\n",
        "\n",
        "# Using word_tokenize to split text into individual words\n",
        "# This is useful for word-level analysis and processing\n",
        "word_tok = word_tokenize(txt)\n",
        "print(f\"\\nWord tokenization:\\n{word_tok}\")\n",
        "\n",
        "# Using RegexpTokenizer to extract only word characters\n",
        "# This is useful when you want to remove punctuation and keep only alphanumeric characters\n",
        "# The pattern r\"\\w+\" matches sequences of alphanumeric characters (letters, digits, and underscores).\n",
        "# It is used here to extract only \"word\" tokens, ignoring punctuation and spaces.\n",
        "tok = RegexpTokenizer(r\"\\w+\")\n",
        "print(f\"\\nRegex tokenization (words only):\\n{tok.tokenize(txt)}\")\n",
        "\n",
        "# Using TreebankWordTokenizer for standard word tokenization\n",
        "# This follows the Penn Treebank tokenization conventions\n",
        "tree_tok = nltk.TreebankWordTokenizer()\n",
        "print(f\"\\nTreebankWordTokenizer:\\n{tree_tok.tokenize(txt)}\")\n",
        "\n",
        "# Using WordPunctTokenizer to split text into words and punctuation\n",
        "# This is useful when you need to preserve punctuation as separate tokens\n",
        "punkt_tok = WordPunctTokenizer()\n",
        "print(f\"\\nWordPunctTokenizer:\\n{punkt_tok.tokenize(txt)}\")\n",
        "\n",
        "# Using simple whitespace tokenization\n",
        "# This is the most basic form of tokenization, splitting on spaces\n",
        "print(f\"\\nWhitespace tokenization:\\n{txt.split()}\")\n",
        "\n",
        "# Using character-level tokenization\n",
        "# This is useful for character-level analysis or when working with non-standard text\n",
        "print(f\"\\nCharacter tokenization:\\n{list(txt)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size: 16.5px; font-weight: bold; color:rgb(7, 213, 240)\">Custom Tokenizer Training</span><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Mr reza. How are you today? I can't stand this weather.\n",
            "The sun is too bright and the temperature is unbearable.\n",
            "I don't know how people can work in these conditions.\n",
            "Maybe we should move to a cooler place.\n",
            "What do you think about that?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Hello!',\n",
              " 'Mr reza.',\n",
              " 'How are you today?',\n",
              " \"I can't stand this weather.\",\n",
              " 'The sun is too bright and the temperature is unbearable.',\n",
              " \"I don't know how people can work in these conditions.\",\n",
              " 'Maybe we should move to a cooler place.',\n",
              " 'What do you think about that?']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk.data                 # Used for loading NLTK resources and models\n",
        "\n",
        "# Load the pre-trained English Punkt tokenizer model\n",
        "punkt_tok = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Open a text file using the correct relative path (adjusted for your project structure)\n",
        "txt_file = open(\"D:/Natural-language-processing/Data/sample_text.txt\", mode='r', encoding='utf-8')\n",
        "\n",
        "txt_read = txt_file.read()\n",
        "print(txt_read)\n",
        "\n",
        "# Tokenize the text using the loaded Punkt tokenizer\n",
        "tok = punkt_tok.tokenize(txt_read)\n",
        "tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw text data from the 'overheard.txt' file in the webtext corpus\n",
        "text_parameter = webtext.raw('overheard.txt')\n",
        "# print(text_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nltk.tokenize.punkt.PunktSentenceTokenizer"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer # Class for sentence tokenization, can be trained on custom data for better accuracy\n",
        "\n",
        "# The PunktSentenceTokenizer is an unsupervised machine learning sentence boundary detection algorithm.\n",
        "# By creating a new instance and passing our own text (text_parameter) to it, we are training the tokenizer\n",
        "# on the specific writing style, abbreviations, and sentence boundaries present in the 'overheard.txt' file.\n",
        "# This allows the tokenizer to better adapt to the nuances of our dataset, potentially improving sentence splitting accuracy\n",
        "# compared to the default pre-trained model.\n",
        "my_tok = PunktSentenceTokenizer(text_parameter)\n",
        "type(my_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pre_token[0]: White guy: So, do you have any plans for this evening?\n",
            "our_token[0]: White guy: So, do you have any plans for this evening?\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text using the pre-trained sent_tokenize function\n",
        "pre_token = sent_tokenize(text_parameter)\n",
        "\n",
        "# Tokenize the text using our custom trained tokenizer\n",
        "our_token = my_tok.tokenize(text_parameter)\n",
        "\n",
        "print(f\"pre_token[0]: {pre_token[0]}\")\n",
        "\n",
        "print(f\"our_token[0]: {our_token[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>Lemma</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Dep</th>\n",
              "      <th>Shape</th>\n",
              "      <th>Is alpha</th>\n",
              "      <th>Is stop</th>\n",
              "      <th>Is punctuation</th>\n",
              "      <th>Head</th>\n",
              "      <th>Children</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Apple</td>\n",
              "      <td>Apple</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>be</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>AUX</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>aux</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>looking</td>\n",
              "      <td>look</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBG</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[Apple, is, at, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>ADP</td>\n",
              "      <td>IN</td>\n",
              "      <td>prep</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>looking</td>\n",
              "      <td>[buying]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buying</td>\n",
              "      <td>buy</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBG</td>\n",
              "      <td>pcomp</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>at</td>\n",
              "      <td>[startup]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U.K.</td>\n",
              "      <td>U.K.</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>X.X.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>startup</td>\n",
              "      <td>startup</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBD</td>\n",
              "      <td>ccomp</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>buying</td>\n",
              "      <td>[U.K., for]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>for</td>\n",
              "      <td>for</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>ADP</td>\n",
              "      <td>IN</td>\n",
              "      <td>prep</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>[billion]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$</td>\n",
              "      <td>$</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>SYM</td>\n",
              "      <td>$</td>\n",
              "      <td>quantmod</td>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>NUM</td>\n",
              "      <td>CD</td>\n",
              "      <td>compound</td>\n",
              "      <td>d</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>billion</td>\n",
              "      <td>billion</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>NUM</td>\n",
              "      <td>CD</td>\n",
              "      <td>pobj</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>for</td>\n",
              "      <td>[$, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>Apple is looking at buying U.K. startup for $1...</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>looking</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Token    Lemma                                           Sentence  \\\n",
              "0     Apple    Apple  Apple is looking at buying U.K. startup for $1...   \n",
              "1        is       be  Apple is looking at buying U.K. startup for $1...   \n",
              "2   looking     look  Apple is looking at buying U.K. startup for $1...   \n",
              "3        at       at  Apple is looking at buying U.K. startup for $1...   \n",
              "4    buying      buy  Apple is looking at buying U.K. startup for $1...   \n",
              "5      U.K.     U.K.  Apple is looking at buying U.K. startup for $1...   \n",
              "6   startup  startup  Apple is looking at buying U.K. startup for $1...   \n",
              "7       for      for  Apple is looking at buying U.K. startup for $1...   \n",
              "8         $        $  Apple is looking at buying U.K. startup for $1...   \n",
              "9         1        1  Apple is looking at buying U.K. startup for $1...   \n",
              "10  billion  billion  Apple is looking at buying U.K. startup for $1...   \n",
              "11        .        .  Apple is looking at buying U.K. startup for $1...   \n",
              "\n",
              "      POS  Tag       Dep  Shape  Is alpha  Is stop  Is punctuation     Head  \\\n",
              "0   PROPN  NNP     nsubj  Xxxxx      True    False           False  looking   \n",
              "1     AUX  VBZ       aux     xx      True     True           False  looking   \n",
              "2    VERB  VBG      ROOT   xxxx      True    False           False  looking   \n",
              "3     ADP   IN      prep     xx      True     True           False  looking   \n",
              "4    VERB  VBG     pcomp   xxxx      True    False           False       at   \n",
              "5   PROPN  NNP     nsubj   X.X.     False    False           False  startup   \n",
              "6    VERB  VBD     ccomp   xxxx      True    False           False   buying   \n",
              "7     ADP   IN      prep    xxx      True     True           False  startup   \n",
              "8     SYM    $  quantmod      $     False    False           False  billion   \n",
              "9     NUM   CD  compound      d     False    False           False  billion   \n",
              "10    NUM   CD      pobj   xxxx      True    False           False      for   \n",
              "11  PUNCT    .     punct      .     False    False            True  looking   \n",
              "\n",
              "              Children  \n",
              "0                   []  \n",
              "1                   []  \n",
              "2   [Apple, is, at, .]  \n",
              "3             [buying]  \n",
              "4            [startup]  \n",
              "5                   []  \n",
              "6          [U.K., for]  \n",
              "7            [billion]  \n",
              "8                   []  \n",
              "9                   []  \n",
              "10              [$, 1]  \n",
              "11                  []  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "proc = spacy.load(\"en_core_web_sm\")\n",
        "doc = proc(text)\n",
        "\n",
        "token_info = []\n",
        "for token in doc:\n",
        "    info = {\n",
        "        \"Token\": token.text,\n",
        "        \"Lemma\": token.lemma_,\n",
        "        \"Sentence\": token.sent.text,\n",
        "        \"POS\": token.pos_,\n",
        "        \"Tag\": token.tag_,\n",
        "        \"Dep\": token.dep_,\n",
        "        \"Shape\": token.shape_,\n",
        "        \"Is alpha\": token.is_alpha,\n",
        "        \"Is stop\": token.is_stop,\n",
        "        \"Is punctuation\": token.is_punct,\n",
        "        \"Head\": token.head.text,\n",
        "        \"Children\": [child.text for child in token.children]\n",
        "    }\n",
        "    token_info.append(info)\n",
        "df = pd.DataFrame(token_info)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Preprocessing:**<br/>\n",
        "Preprocessing in NLP typically involves several key steps to clean and standardize text data before analysis or modeling. The main preprocessing steps are:<br/>\n",
        "▪ `Lowercasing:` Convert all text to lowercase to ensure uniformity (e.g., \"Hello\" and \"hello\").<br/>\n",
        "▪ `Punctuation & Special Character Removal:` Remove punctuation marks, non-alphanumeric symbols, and special characters to focus on meaningful words.<br/>\n",
        "▪ `Tokenization:` Divide text into smaller units—such as sentences or words—to simplify processing and analysis. This step impacts model size, training efficiency, and how well models interpret language.<br/>\n",
        "▪ `Stop-Word Removal:` Remove common words (like \"the\", \"is\", \"and\") that do not add significant meaning.<br/>\n",
        "▪ `Stemming and Lemmatization:` Reduce words to their root or base form (e.g., \"running\" → \"run\").<br/> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text (First 500 characters):\n",
            " [the tragedie of hamlet by william shakespeare 1599]   actus primus. scoena prima.  enter barnardo and francisco two centinels.    barnardo. who's there?   fran. nay answer me: stand & vnfold your selfe     bar. long liue the king     fran. barnardo?   bar. he     fran. you come most carefully vpon your houre     bar. 'tis now strook twelue, get thee to bed francisco     fran. for this releefe much thankes: 'tis bitter cold, and i am sicke at heart     barn. haue you had quiet guard?   fran. not\n",
            "\n",
            "Stemmed Text (First 500 characters):\n",
            " tragedi hamlet william shakespear 1599 actu primu scoena prima enter barnardo francisco two centinel barnardo fran nay answer stand vnfold self bar long liue king fran barnardo bar fran come care vpon hour bar strook twelu get thee bed francisco fran releef much thank bitter cold sick heart barn haue quiet guard fran mous stir barn well goodnight meet horatio marcellu riual watch bid make hast enter horatio marcellu fran think hear stand hor friend ground mar dane fran giue good night mar farwel\n",
            "\n",
            "Lemmatized Text (First 500 characters):\n",
            " tragedie hamlet william shakespeare 1599 actus primus scoena prima enter barnardo francisco two centinels barnardo fran nay answer stand vnfold selfe bar long liue king fran barnardo bar fran come carefully vpon houre bar strook twelue get thee bed francisco fran releefe much thankes bitter cold sicke heart barn haue quiet guard fran mouse stirring barn well goodnight meet horatio marcellus riuals watch bid make hast enter horatio marcellus fran thinke heare stand hor friend ground mar dane fran\n"
          ]
        }
      ],
      "source": [
        "# Select a text file from Gutenberg (e.g., 'shakespeare-hamlet.txt')\n",
        "file_id = \"shakespeare-hamlet.txt\"\n",
        "raw_text = gutenberg.raw(file_id)\n",
        "\n",
        "# Step 1: Text Cleaning (Removing Gutenberg Header/Footer)\n",
        "def clean_text(text):\n",
        "    lines = text.split(\"\\n\") # break (enter - new line)\n",
        "    start_idx, end_idx = 0, len(lines)\n",
        "\n",
        "    # Removing Gutenberg boilerplate (First few and last few lines)\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"START OF THIS PROJECT GUTENBERG\" in line:\n",
        "            start_idx = i + 1\n",
        "        if \"END OF THIS PROJECT GUTENBERG\" in line:\n",
        "            end_idx = i\n",
        "            break\n",
        "\n",
        "    cleaned_lines = lines[start_idx:end_idx]\n",
        "    cleaned_text = \" \".join(cleaned_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "text = clean_text(raw_text)\n",
        "\n",
        "# Step 2: Lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 4: Remove Punctuation & Stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Step 5: Stemming & Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Step 6: Convert back to text\n",
        "stemmed_text = \" \".join(stemmed_tokens)\n",
        "lemmatized_text = \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Text (First 500 characters):\\n\", text[:500])\n",
        "print(\"\\nStemmed Text (First 500 characters):\\n\", stemmed_text[:500])\n",
        "print(\"\\nLemmatized Text (First 500 characters):\\n\", lemmatized_text[:500])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text-Introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
