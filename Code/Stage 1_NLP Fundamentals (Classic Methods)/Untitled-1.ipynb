{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold\">Welcome to Natural Language Processing (NLP) in Python</span><br/>\n",
        "Presented by: Reza Saadatyar (2024-2025)<br/>\n",
        "E-mail: Reza.Saadatyar@outlook.com<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold\">Outline</span><br/>\n",
        "▪ Introduction to NLP<br/>\n",
        "▪ Text Dataset Structure<br/>\n",
        "▪ Text Preprocessing<br/>\n",
        "▪ Tokenization Concepts<br/>\n",
        "▪ Stopwords Removal<br/>\n",
        "▪ Regular Expressions in NLP<br/>\n",
        "▪ Part-of-Speech (POS) Tagging<br/>\n",
        "▪ Semantic, Syntactic, and Sentiment Analysis<br/>\n",
        "▪ Chunking and Parsing<br/>\n",
        "▪ Named Entity Recognition (NER)<br/>\n",
        "▪ Hypernyms and Hyponyms<br/>\n",
        "▪ Stemming and Lemmatization<br/>\n",
        "▪ Custom Tokenizer Training<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(255, 251, 18)\">Introduction to NLP</span><br/>\n",
        "NLP is a branch of artificial intelligence that allows computers to understand, interpret, and generate human language by combining linguistics, computer science, and machine learning.<br/>\n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold\">Key Areas of NLP:</span><br/>\n",
        "`Text Analysis:`<br/>\n",
        "▪ Tokenization: Breaking text into words or sentences.<br/>\n",
        "▪ Part-of-Speech (POS) Tagging: Identifying grammatical components (e.g., nouns, verbs).<br/>\n",
        "▪ Named Entity Recognition (NER): Extracting entities like names, dates, or organizations.<br/>\n",
        "▪ Sentiment Analysis: Determining the emotional tone (positive, negative, neutral).<br/>\n",
        "\n",
        "`Language Generation:`<br/>\n",
        "▪ Text Summarization: Condensing long texts into shorter summaries.<br/>\n",
        "▪ Machine Translation: Converting text between languages (e.g., Google Translate).<br/>\n",
        "▪ Text Generation: Creating human-like text (e.g., chatbots, story generators).<br/>\n",
        "\n",
        "`Speech Processing:`<br/>\n",
        "▪ Speech Recognition: Converting spoken words to text (e.g., Siri, Alexa).<br/>\n",
        "▪ Text-to-Speech (TTS): Generating spoken language from text.<br/>\n",
        "▪ Voice Assistants: Combining speech recognition and NLP for interactive systems.<br/>\n",
        "\n",
        "`Semantic Understanding:`<br/>\n",
        "▪ Word Embeddings: Representing words as vectors (e.g., Word2Vec, BERT).<br/>\n",
        "▪ Question Answering: Providing precise answers to user queries.<br/>\n",
        "▪ Dialogue Systems: Enabling conversational agents to maintain context.<br/>\n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold\">NLP Challenges:</span><br/>\n",
        "▪ `Ambiguity & Context Sensitivity:` Words or sentences with multiple meanings.<br/>\n",
        "▪ `Cultural & Linguistic Nuances:` Variations in idioms, slang, and grammar.<br/>\n",
        "▪ `Handling Massive Datasets:` Processing large volumes of text efficiently.<br/>\n",
        "▪ `Continuous Innovation:` Keeping up with evolving models and data sources.<br/>\n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold\">NLTK and spaCy Libraries</span><br/>\n",
        "▪ [NLTK](https://www.nltk.org/): Open-source Python library for educational and research purposes, offering tools for tokenization, stemming, lemmatization, and more.<br/>\n",
        "▪ [spaCy](https://spacy.io/): Industrial-strength NLP library for real-world applications, optimized for speed and accuracy.<br/>\n",
        "\n",
        "<span style=\"font-size:16px; font-weight:bold\">NLTK & spaCy Setup</span><br/>\n",
        "▪ `NLTK's Punkt:` Unsupervised sentence tokenizer.<br/>\n",
        "▪ `spaCy's en_core_web_sm:` Small English model for tokenization, POS tagging, and NER.<br/>\n",
        "▪ `WordNet:` Lexical database for semantic analysis.<br/>\n",
        "▪ `Gutenberg Corpus:` Collection of classic literary texts.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install spacy\n",
        "! pip install nltk\n",
        "! pip install regex==2023.10.3\n",
        "! pip install spacy-wordnet\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import webtext # Provides access to the Webtext corpus, useful for training and testing tokenizers\n",
        "\n",
        "from nltk.corpus import  gutenberg, wordnet, stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer, TreebankWordTokenizer, RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# from spacy.matcher import Matcher\n",
        "\n",
        "# nltk.download('punkt')      # Download the pre-trained sentence tokenizer model ('punkt')\n",
        "# nltk.download('punkt_tab')  # Download additional tokenizer resources for handling special cases ('punkt_tab')\n",
        "# nltk.download('wordnet')    # Download the WordNet lexical database for lemmatization\n",
        "# nltk.download('webtext')    # Download the Webtext corpus containing diverse text samples\n",
        "# nltk.download('gutenberg')  # Download the Gutenberg corpus for text samples\n",
        "# nltk.download('stopwords')  # Download the list of common stopwords\n",
        "# nltk.download('vader_lexicon')  # Download the VADER sentiment analysis lexicon\n",
        "# nltk.download('averaged_perceptron_tagger_eng')  # Download the averaged perceptron tagger for POS tagging\n",
        "# nltk.download('averaged_perceptron_tagger', quiet=True)  # Download the averaged perceptron tagger for POS tagging\n",
        "# !python -m spacy download en_core_web_sm  # Download the small English model for spaCy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(11, 7, 241)\">Text Dataset Structure</span><br/>\n",
        "▪ `Corpora:` Large, structured sets of texts for linguistic analysis or model training.<br/>\n",
        "▪ `Corpus:` A single collection of documents.<br/>\n",
        "▪ `Document:` An individual piece of text (e.g., article, tweet).<br/>\n",
        "▪ `Token:` Smallest unit of text (e.g., word, punctuation) after tokenization.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents in the webtext corpus:\n",
            "• firefox.txt\n",
            "• grail.txt\n",
            "• overheard.txt\n",
            "• pirates.txt\n",
            "• singles.txt\n",
            "• wine.txt\n"
          ]
        }
      ],
      "source": [
        "corpus = webtext # The entire webtext corpus\n",
        "documents = corpus.fileids()   # List all documents (fileids) in the corpus\n",
        "print(\"Documents in the webtext corpus:\")\n",
        "for doc in documents:\n",
        "    print(f\"• {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Document: firefox.txt\n",
            "\n",
            "First 300 characters of the document:\n",
            "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
            "When in full screen mode\n",
            "Pressing Ctrl-N should open a new browser when only download dialog is left open\n",
            "add icons to context menu\n",
            "So called \"tab bar\" should be made a proper toolbar or given \n"
          ]
        }
      ],
      "source": [
        "document_id = documents[0]  # Select a document (e.g., 'grail.txt')\n",
        "print(f\"Selected Document: {document_id}\")\n",
        "\n",
        "raw_text = corpus.raw(document_id)  # Get the raw text of the document\n",
        "print(f\"\\nFirst 300 characters of the document:\\n{raw_text[:300]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(255, 165, 0)\">Text Preprocessing</span><br/>\n",
        "Preprocessing involves cleaning and standardizing text data for analysis or modeling.<br/>\n",
        "▪ `Lowercasing:` Convert text to lowercase for uniformity.<br/>\n",
        "▪ `Punctuation & Special Character Removal:` Remove non-alphanumeric symbols.<br/>\n",
        "▪ `Tokenization:` Split text into sentences or words.<br/>\n",
        "▪ `Stop-Word Removal:` Remove common, low-meaning words.<br/>\n",
        "▪ `Stemming/Lemmatization:` Reduce words to their root or base form.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text (First 500 characters):\n",
            " [the tragedie of hamlet by william shakespeare 1599]   actus primus. scoena prima.  enter barnardo and francisco two centinels.    barnardo. who's there?   fran. nay answer me: stand & vnfold your selfe     bar. long liue the king     fran. barnardo?   bar. he     fran. you come most carefully vpon your houre     bar. 'tis now strook twelue, get thee to bed francisco     fran. for this releefe much thankes: 'tis bitter cold, and i am sicke at heart     barn. haue you had quiet guard?   fran. not\n",
            "\n",
            "Stemmed Text (First 500 characters):\n",
            " tragedi hamlet william shakespear 1599 actu primu scoena prima enter barnardo francisco two centinel barnardo fran nay answer stand vnfold self bar long liue king fran barnardo bar fran come care vpon hour bar strook twelu get thee bed francisco fran releef much thank bitter cold sick heart barn haue quiet guard fran mous stir barn well goodnight meet horatio marcellu riual watch bid make hast enter horatio marcellu fran think hear stand hor friend ground mar dane fran giue good night mar farwel\n",
            "\n",
            "Lemmatized Text (First 500 characters):\n",
            " tragedie hamlet william shakespeare 1599 actus primus scoena prima enter barnardo francisco two centinels barnardo fran nay answer stand vnfold selfe bar long liue king fran barnardo bar fran come carefully vpon houre bar strook twelue get thee bed francisco fran releefe much thankes bitter cold sicke heart barn haue quiet guard fran mouse stirring barn well goodnight meet horatio marcellus riuals watch bid make hast enter horatio marcellus fran thinke heare stand hor friend ground mar dane fran\n"
          ]
        }
      ],
      "source": [
        "file_id = 'shakespeare-hamlet.txt'\n",
        "raw_text = gutenberg.raw(file_id)\n",
        "\n",
        "def clean_text(text):\n",
        "    lines = text.split('\\n')\n",
        "    start_idx, end_idx = 0, len(lines)\n",
        "    for i, line in enumerate(lines):\n",
        "        if 'START OF THIS PROJECT GUTENBERG' in line:\n",
        "            start_idx = i + 1\n",
        "        if 'END OF THIS PROJECT GUTENBERG' in line:\n",
        "            end_idx = i\n",
        "            break\n",
        "    cleaned_lines = lines[start_idx:end_idx]\n",
        "    cleaned_text = ' '.join(cleaned_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "text = clean_text(raw_text)\n",
        "text = text.lower()\n",
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "stemmed_text = ' '.join(stemmed_tokens)\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "print('Original Text (First 500 characters):\\n', text[:500])\n",
        "print('\\nStemmed Text (First 500 characters):\\n', stemmed_text[:500])\n",
        "print('\\nLemmatized Text (First 500 characters):\\n', lemmatized_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(255, 0, 157)\">4️⃣ Tokenization Concepts</span><br/>\n",
        "Tokenization breaks text into smaller units (tokens) for processing.<br/>\n",
        "▪ Sentence Tokenization: Splits text into sentences.<br/>\n",
        "▪ Word Tokenization: Breaks text into words.<br/>\n",
        "▪ Regex Tokenization: Extracts patterns using regular expressions.<br/>\n",
        "▪ Treebank Tokenization: Follows Penn Treebank conventions.<br/>\n",
        "▪ WordPunct Tokenization: Separates words and punctuation.<br/>\n",
        "▪ Whitespace Tokenization: Splits on spaces.<br/>\n",
        "▪ Character Tokenization: Breaks into individual characters.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(raw_text)\n",
        "print(f'Number of sentences in the document: {len(sentences)}')\n",
        "print(f'\\nFirst sentence:\\n{sentences[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = word_tokenize(raw_text)\n",
        "print(f'Number of tokens in the document: {len(tokens)}')\n",
        "print(f'First 10 tokens:\\n{tokens[:10]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = \"I am learning Natural Language Processing. I'm learning Python programming. It is very user friendly. I'm ready to start coding.\"\n",
        "print(f'Sentence tokenization:\\n{sent_tokenize(txt)}')\n",
        "print(f'\\nWord tokenization:\\n{word_tokenize(txt)}')\n",
        "tok = RegexpTokenizer(r'\\w+')\n",
        "print(f'\\nRegex tokenization (words only):\\n{tok.tokenize(txt)}')\n",
        "tree_tok = TreebankWordTokenizer()\n",
        "print(f'\\nTreebankWordTokenizer:\\n{tree_tok.tokenize(txt)}')\n",
        "punkt_tok = WordPunctTokenizer()\n",
        "print(f'\\nWordPunctTokenizer:\\n{punkt_tok.tokenize(txt)}')\n",
        "print(f'\\nWhitespace tokenization:\\n{txt.split()}')\n",
        "print(f'\\nCharacter tokenization:\\n{list(txt)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(6, 168, 243)\">5️⃣ Stopwords Removal</span><br/>\n",
        "Stopwords are common words (e.g., 'the', 'is', 'and') removed to reduce noise and improve analysis efficiency.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'This is an example sentence showing off the stop words filtration.'\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "print('Filtered Words:', filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = nlp(text)\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "print('Original Text:')\n",
        "print(text)\n",
        "print('\\nAfter Stopwords Removal:')\n",
        "print(' '.join(filtered_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(237, 4, 245)\">6️⃣ Regular Expressions in NLP</span><br/>\n",
        "Regular expressions (regex) identify, extract, and manipulate text patterns.<br/>\n",
        "▪ Pattern Matching: Find sequences like emails or dates.<br/>\n",
        "▪ Substring Extraction: Extract specific text parts.<br/>\n",
        "▪ Pattern Replacement/Removal: Substitute or remove patterns.<br/>\n",
        "▪ Noise Filtering: Clean irrelevant data.<br/>\n",
        "\n",
        "<span style=\"font-size:15.5px; font-weight:bold\">Character Ranges and Quantifiers:</span><br/>\n",
        "▪ [A-Za-z]: Matches any letter.<br/>\n",
        "▪ {2}: Exactly 2 occurrences.<br/>\n",
        "▪ \\d{3}: Exactly 3 digits.<br/>\n",
        "▪ []: Define a character set/range.<br/>\n",
        "\n",
        "<span style=\"font-size:15.5px; font-weight:bold\">Example Email Regex:</span><br/>\n",
        "^[\\w\\.-]+@([\\w-]+\\.)+[\\w-]{2,4}$<br/>\n",
        "▪ ^[\\w\\.-]+: Username (word characters, dots, hyphens).<br/>\n",
        "▪ @: Literal '@' symbol.<br/>\n",
        "▪ ([\\w-]+\\.)+: Domain/subdomains.<br/>\n",
        "▪ [\\w-]{2,4}$: Top-level domain (2-4 characters).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_text = 'The quick brown fox jumps over the lazy dog, 123-252 times!'\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(example_text)\n",
        "print('NLTK Regex Tokens:', tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_emails = 'Contact us at admin.support_34@example.com or sales-dep@company.org for inquiries.'\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "emails = re.findall(email_pattern, text_emails)\n",
        "print('Detected Emails:', emails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{'TEXT': {'REGEX': '^[A-Z][a-z]+'}}]\n",
        "matcher.add('CAPITALIZED_WORD', [pattern])\n",
        "text = 'Alice and Bob went to New York City last Friday.'\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print('Capitalized words found in the sentence:')\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print('•', span.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(12, 238, 125)\">7️⃣ Part-of-Speech (POS) Tagging</span><br/>\n",
        "POS tagging labels words with their grammatical roles (e.g., noun, verb).<br/>\n",
        "▪ Noun (N): Names of people, places, things.<br/>\n",
        "▪ Verb (V): Actions or states.<br/>\n",
        "▪ Adjective (ADJ): Describes nouns.<br/>\n",
        "▪ Adverb (ADV): Modifies verbs, adjectives, or adverbs.<br/>\n",
        "▪ Preposition (P): Shows relationships (e.g., at, on, in).<br/>\n",
        "▪ Conjunction (CON): Connects clauses/words (e.g., and, or).<br/>\n",
        "▪ Pronoun (PRO): Replaces nouns (e.g., you, I).<br/>\n",
        "▪ Interjection (INT): Expresses emotion (e.g., Wow!, Oh!).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = 'The quick brown fox jumps over the lazy dog.'\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(f'POS Tags using NLTK:\\n{pos_tags}')\n",
        "doc = nlp(sentence)\n",
        "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(f'\\nPOS Tags using spaCy:\\n{spacy_pos_tags}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(38, 255, 18)\">8️⃣ Semantic, Syntactic, and Sentiment Analysis</span><br/>\n",
        "▪ Semantic Analysis: Understands meaning in context.<br/>\n",
        "▪ Syntactic Analysis: Examines grammatical structure.<br/>\n",
        "▪ Sentiment Analysis: Determines emotional tone (positive, negative, neutral).<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word = 'bank'\n",
        "synsets = wordnet.synsets(word)\n",
        "print(f'Semantic Analysis: Synsets for \\'{word}\\':')\n",
        "for syn in synsets:\n",
        "    print(f'• {syn.name()}: {syn.definition()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = 'The quick brown fox jumps over the lazy dog.'\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(f'Syntactic Analysis: POS Tags for the sentence:\\n{pos_tags}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "text = 'I love natural language processing! It\\'s amazing and fun.'\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(f'Sentiment Analysis: Sentiment scores for the text:\\n{sentiment_scores}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(171, 12, 245)\">9️⃣ Chunking and Parsing</span><br/>\n",
        "▪ Chunking: Groups words into meaningful phrases (e.g., noun phrases).<br/>\n",
        "▪ Parsing: Analyzes grammatical structure and sentence hierarchy.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = 'The quick brown fox jumps over the lazy dog.'\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "grammar = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "tree = cp.parse(pos_tags)\n",
        "print('Chunked Sentence Structure:')\n",
        "print(tree)\n",
        "tree.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sen = nlp(sentence)\n",
        "spacy_noun_chunks = [chunk.text for chunk in sen.noun_chunks]\n",
        "print(f'spaCy Noun Chunks List:\\n{spacy_noun_chunks}')\n",
        "nltk_noun_chunks = []\n",
        "for subtree in tree.subtrees():\n",
        "    if subtree.label() == 'NP':\n",
        "        chunk = ' '.join(word for word, pos in subtree.leaves())\n",
        "        nltk_noun_chunks.append(chunk)\n",
        "print(f'\\nNLTK Noun Chunks List:\\n{nltk_noun_chunks}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(171, 12, 245)\">🔟 Named Entity Recognition (NER)</span><br/>\n",
        "NER identifies and classifies entities (e.g., person, organization, location) in text.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'Barack Obama was born in Hawaii and served as the 44th President of the United States.'\n",
        "doc = nlp(text)\n",
        "print('spaCy NER Results:')\n",
        "for ent in doc.ents:\n",
        "    print(f'• {ent.text} ({ent.label_})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:#eb5e28\">1️⃣1️⃣ Hypernyms and Hyponyms</span><br/>\n",
        "▪ Hypernyms: General category terms (e.g., 'animal' for 'dog').<br/>\n",
        "▪ Hyponyms: Specific instances (e.g., 'poodle' for 'dog').<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word = 'dog'\n",
        "synsets = wordnet.synsets(word, pos=wordnet.NOUN)\n",
        "if synsets:\n",
        "    syn = synsets[0]\n",
        "    print(f'Synset for \\'{word}\\': {syn.name()} - {syn.definition()}')\n",
        "    hypernyms = syn.hypernyms()\n",
        "    print('\\nHypernyms:')\n",
        "    for h in hypernyms:\n",
        "        print(f'• {h.name()} - {h.definition()}')\n",
        "    hyponyms = syn.hyponyms()\n",
        "    print('\\nHyponyms:')\n",
        "    for h in hyponyms[:5]:\n",
        "        print(f'• {h.name()} - {h.definition()}')\n",
        "else:\n",
        "    print(f'No synsets found for \\'{word}\\'.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:#cbaf89\">1️⃣2️⃣ Stemming and Lemmatization</span><br/>\n",
        "▪ Stemming: Removes suffixes to get root form (e.g., 'playing' → 'play').<br/>\n",
        "▪ Lemmatization: Reduces to dictionary form, considering context (e.g., 'better' → 'good').<br/>\n",
        "\n",
        "<span style=\"font-size:15.5px; font-weight:bold\">Comparison Table:</span><br/>\n",
        "| Word      | Lancaster Stem | Porter Stem | Lemma      |\n",
        "|-----------|:--------------|:------------|:-----------|\n",
        "| playing   | play           | play        | play       |\n",
        "| played    | play           | play        | play       |\n",
        "| plays     | play           | play        | play       |\n",
        "| better    | bet            | better      | good       |\n",
        "| running   | run            | run         | run        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = ['Cats are running', 'Dogs played outside']\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "results = []\n",
        "for doc in documents:\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    results.append({'original': doc, 'tokens': tokens, 'stems': stems, 'lemmas': lemmas})\n",
        "df = pd.DataFrame(results)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_words = ['playing', 'played', 'plays', 'better', 'running']\n",
        "comparison = []\n",
        "for word in sample_words:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    comparison.append({'word': word, 'stem': stem, 'lemma': lemma})\n",
        "print('\\nStemming vs Lemmatization Comparison:')\n",
        "df = pd.DataFrame(comparison)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spacy_docs = ['Cats are running', 'Dogs played outside']\n",
        "spacy_results = []\n",
        "for doc in spacy_docs:\n",
        "    spacy_doc = nlp(doc)\n",
        "    tokens = [token.text for token in spacy_doc]\n",
        "    lemmas = [token.lemma_ for token in spacy_doc]\n",
        "    pos = [token.pos_ for token in spacy_doc]\n",
        "    spacy_results.append({'original': doc, 'tokens': tokens, 'lemmas': lemmas, 'pos': pos})\n",
        "spacy_df = pd.DataFrame(spacy_results)\n",
        "print('\\nspaCy Lemmatization and POS Tagging:')\n",
        "spacy_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:16px; font-weight:bold; color:rgb(7, 213, 240)\">1️⃣3️⃣ Custom Tokenizer Training</span><br/>\n",
        "Training custom tokenizers improves accuracy for specific datasets.<br/>\n",
        "Note: The following cell is commented out as it requires a local file. Replace the file path with your own text file or use an available corpus.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import nltk.data\n",
        "# punkt_tok = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
        "# txt_file = open('sample_text.txt', mode='r', encoding='utf-8')\n",
        "# txt_read = txt_file.read()\n",
        "# print(txt_read)\n",
        "# tok = punkt_tok.tokenize(txt_read)\n",
        "# txt_file.close()\n",
        "# tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_parameter = webtext.raw('overheard.txt')\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "my_tok = PunktSentenceTokenizer(text_parameter)\n",
        "pre_token = sent_tokenize(text_parameter)\n",
        "our_token = my_tok.tokenize(text_parameter)\n",
        "print(f'pre_token[0]: {pre_token[0]}')\n",
        "print(f'our_token[0]: {our_token[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'Apple is looking at buying U.K. startup for $1 billion.'\n",
        "doc = nlp(text)\n",
        "results = []\n",
        "for token in doc:\n",
        "    results.append({\n",
        "        'Token': token.text,\n",
        "        'Lemma': token.lemma_,\n",
        "        'Sentence': str(token.sent),\n",
        "        'POS': token.pos_,\n",
        "        'Tag': token.tag_,\n",
        "        'Dep': token.dep_,\n",
        "        'Shape': token.shape_,\n",
        "        'Is alpha': token.is_alpha,\n",
        "        'Is stop': token.is_stop,\n",
        "        'Is punctuation': token.is_punct,\n",
        "        'Head': token.head.text,\n",
        "        'Children': [child.text for child in token.children]\n",
        "    })\n",
        "df = pd.DataFrame(results)\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
